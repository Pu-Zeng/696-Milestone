{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN Emotion Regression Model with MIDI data#\n",
    "# Last editted by Pu Zeng, 19/10/2023 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "u8eSS9tg62ge",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# import mido\n",
    "import string\n",
    "import numpy as np\n",
    "# from utilis import get_pianoroll_data\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# import mido\n",
    "import string\n",
    "import numpy as np\n",
    "# from utilis import get_pianoroll_data\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All content numbers is 1\n",
      "Total number of muisc 20\n",
      "Total number of muisc 41\n",
      "Total number of muisc 61\n",
      "Total number of muisc 82\n",
      "Total number of muisc 102\n",
      "Total number of muisc 123\n",
      "Total number of muisc 143\n",
      "Total number of muisc 164\n",
      "Total number of muisc 184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 29.41it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.11s/it]\n",
      "2it [00:01,  1.26it/s]\n",
      "3it [00:02,  1.44it/s]\n",
      "4it [00:02,  1.44it/s]C:\\Milestone 2\\Milestone 2\\696-Milestone\\Data_Preprocessing\\Emotion_MIDI2.py:73: RuntimeWarning: invalid value encountered in divide\n",
      "  roll = (roll/count).T\n",
      "\n",
      "5it [00:03,  1.54it/s]\n",
      "6it [00:04,  1.51it/s]\n",
      "7it [00:05,  1.36it/s]\n",
      "8it [00:06,  1.18it/s]\n",
      "9it [00:07,  1.01it/s]\n",
      "10it [00:08,  1.00s/it]\n",
      "11it [00:09,  1.14it/s]\n",
      "12it [00:09,  1.44it/s]\n",
      "13it [00:10,  1.41it/s]\n",
      "14it [00:10,  1.37it/s]\n",
      "15it [00:12,  1.08it/s]D:\\programing\\anaconda3\\lib\\site-packages\\pretty_midi\\pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "\n",
      "16it [00:12,  1.27it/s]\n",
      "17it [00:13,  1.42it/s]\n",
      "18it [00:13,  1.55it/s]\n",
      "19it [00:14,  1.33it/s]\n",
      "20it [00:15,  1.28it/s]\n",
      "20it [00:15,  1.28it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.87it/s]\n",
      "2it [00:01,  1.26it/s]\n",
      "3it [00:02,  1.12it/s]\n",
      "4it [00:03,  1.10it/s]\n",
      "5it [00:04,  1.26it/s]\n",
      "6it [00:05,  1.14it/s]\n",
      "7it [00:06,  1.12it/s]\n",
      "8it [00:06,  1.17it/s]\n",
      "9it [00:08,  1.01s/it]\n",
      "10it [00:09,  1.03s/it]\n",
      "11it [00:10,  1.05s/it]\n",
      "12it [00:11,  1.03s/it]\n",
      "13it [00:12,  1.00s/it]\n",
      "14it [00:13,  1.01it/s]\n",
      "15it [00:14,  1.01s/it]\n",
      "16it [00:14,  1.08it/s]\n",
      "17it [00:15,  1.33it/s]\n",
      "18it [00:15,  1.37it/s]\n",
      "19it [00:16,  1.35it/s]\n",
      "20it [00:17,  1.46it/s]\n",
      "21it [00:17,  1.55it/s]\n",
      "21it [00:17,  1.18it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.19s/it]\n",
      "2it [00:01,  1.10it/s]\n",
      "3it [00:02,  1.25it/s]\n",
      "4it [00:03,  1.18it/s]\n",
      "5it [00:04,  1.36it/s]\n",
      "6it [00:04,  1.26it/s]\n",
      "7it [00:05,  1.35it/s]\n",
      "8it [00:06,  1.33it/s]\n",
      "9it [00:06,  1.40it/s]\n",
      "10it [00:08,  1.04it/s]\n",
      "11it [00:09,  1.12s/it]\n",
      "12it [00:11,  1.11s/it]\n",
      "13it [00:12,  1.08s/it]\n",
      "14it [00:12,  1.05it/s]\n",
      "15it [00:13,  1.07it/s]\n",
      "16it [00:14,  1.20it/s]\n",
      "17it [00:15,  1.08it/s]\n",
      "18it [00:16,  1.02s/it]\n",
      "19it [00:17,  1.03it/s]\n",
      "20it [00:18,  1.06s/it]\n",
      "20it [00:18,  1.07it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.13s/it]\n",
      "2it [00:01,  1.10it/s]\n",
      "3it [00:03,  1.02s/it]\n",
      "4it [00:03,  1.08it/s]\n",
      "5it [00:04,  1.02it/s]\n",
      "6it [00:05,  1.14it/s]\n",
      "7it [00:06,  1.01it/s]\n",
      "8it [00:07,  1.09it/s]\n",
      "9it [00:08,  1.05it/s]\n",
      "10it [00:08,  1.30it/s]\n",
      "11it [00:09,  1.21it/s]\n",
      "12it [00:10,  1.27it/s]\n",
      "13it [00:11,  1.34it/s]\n",
      "14it [00:12,  1.31it/s]\n",
      "15it [00:13,  1.17it/s]\n",
      "16it [00:13,  1.35it/s]\n",
      "17it [00:14,  1.23it/s]\n",
      "18it [00:15,  1.22it/s]\n",
      "19it [00:16,  1.25it/s]\n",
      "20it [00:16,  1.42it/s]\n",
      "21it [00:17,  1.31it/s]\n",
      "21it [00:17,  1.20it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  1.01it/s]\n",
      "2it [00:01,  1.16it/s]\n",
      "3it [00:01,  1.77it/s]\n",
      "4it [00:02,  1.64it/s]\n",
      "5it [00:03,  1.55it/s]\n",
      "6it [00:04,  1.43it/s]\n",
      "7it [00:04,  1.41it/s]\n",
      "8it [00:06,  1.12it/s]\n",
      "9it [00:07,  1.08it/s]\n",
      "10it [00:07,  1.18it/s]\n",
      "11it [00:08,  1.23it/s]\n",
      "12it [00:09,  1.28it/s]\n",
      "13it [00:10,  1.04s/it]\n",
      "14it [00:11,  1.03it/s]\n",
      "15it [00:13,  1.19s/it]\n",
      "16it [00:14,  1.11s/it]\n",
      "17it [00:15,  1.12s/it]\n",
      "18it [00:16,  1.11s/it]\n",
      "19it [00:17,  1.03s/it]\n",
      "20it [00:17,  1.21it/s]\n",
      "20it [00:17,  1.12it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:02,  2.35s/it]\n",
      "2it [00:03,  1.60s/it]\n",
      "3it [00:04,  1.15s/it]\n",
      "4it [00:05,  1.15s/it]\n",
      "5it [00:05,  1.07it/s]\n",
      "6it [00:06,  1.13it/s]\n",
      "7it [00:07,  1.27it/s]\n",
      "8it [00:07,  1.27it/s]\n",
      "9it [00:08,  1.43it/s]\n",
      "10it [00:09,  1.33it/s]\n",
      "11it [00:10,  1.29it/s]\n",
      "12it [00:11,  1.14it/s]\n",
      "13it [00:12,  1.15it/s]\n",
      "14it [00:12,  1.19it/s]\n",
      "15it [00:13,  1.27it/s]\n",
      "16it [00:14,  1.21it/s]\n",
      "17it [00:15,  1.25it/s]\n",
      "18it [00:15,  1.68it/s]\n",
      "19it [00:15,  1.62it/s]\n",
      "20it [00:16,  1.52it/s]\n",
      "21it [00:18,  1.11it/s]\n",
      "21it [00:18,  1.16it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.40s/it]\n",
      "2it [00:01,  1.36it/s]\n",
      "3it [00:02,  1.44it/s]\n",
      "4it [00:03,  1.45it/s]\n",
      "5it [00:03,  1.33it/s]\n",
      "6it [00:04,  1.29it/s]\n",
      "7it [00:05,  1.32it/s]\n",
      "8it [00:06,  1.38it/s]\n",
      "9it [00:06,  1.44it/s]\n",
      "10it [00:07,  1.72it/s]\n",
      "11it [00:07,  2.02it/s]\n",
      "12it [00:08,  1.37it/s]\n",
      "13it [00:09,  1.50it/s]\n",
      "14it [00:10,  1.27it/s]\n",
      "15it [00:10,  1.56it/s]\n",
      "16it [00:11,  1.42it/s]\n",
      "17it [00:12,  1.23it/s]\n",
      "18it [00:13,  1.30it/s]\n",
      "19it [00:13,  1.36it/s]\n",
      "20it [00:14,  1.37it/s]\n",
      "20it [00:14,  1.39it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:01,  1.36s/it]\n",
      "2it [00:01,  1.34it/s]\n",
      "3it [00:03,  1.43s/it]\n",
      "4it [00:04,  1.08s/it]\n",
      "5it [00:05,  1.03it/s]\n",
      "6it [00:06,  1.07it/s]\n",
      "7it [00:06,  1.16it/s]\n",
      "8it [00:07,  1.13it/s]\n",
      "9it [00:09,  1.03s/it]\n",
      "10it [00:09,  1.04it/s]\n",
      "11it [00:11,  1.23s/it]\n",
      "12it [00:12,  1.05it/s]\n",
      "13it [00:12,  1.22it/s]\n",
      "14it [00:13,  1.32it/s]\n",
      "15it [00:13,  1.45it/s]\n",
      "16it [00:14,  1.25it/s]\n",
      "17it [00:15,  1.35it/s]\n",
      "18it [00:16,  1.13it/s]\n",
      "19it [00:17,  1.03it/s]\n",
      "20it [00:19,  1.31s/it]\n",
      "21it [00:20,  1.25s/it]\n",
      "21it [00:20,  1.00it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  4.19it/s]\n",
      "2it [00:01,  1.20it/s]\n",
      "3it [00:02,  1.20it/s]\n",
      "4it [00:03,  1.07s/it]\n",
      "5it [00:04,  1.06s/it]\n",
      "6it [00:05,  1.02it/s]\n",
      "7it [00:05,  1.38it/s]\n",
      "8it [00:06,  1.48it/s]\n",
      "9it [00:07,  1.44it/s]\n",
      "10it [00:07,  1.60it/s]\n",
      "11it [00:08,  1.30it/s]\n",
      "12it [00:09,  1.48it/s]\n",
      "13it [00:10,  1.29it/s]\n",
      "14it [00:11,  1.23it/s]\n",
      "15it [00:11,  1.37it/s]\n",
      "16it [00:11,  1.60it/s]\n",
      "17it [00:12,  1.47it/s]\n",
      "18it [00:13,  1.34it/s]\n",
      "19it [00:14,  1.30it/s]\n",
      "20it [00:15,  1.11it/s]\n",
      "20it [00:15,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "!python ../Data_Preprocessing/Emotion_MIDI2.py ../Toy_Dataset/Lakh/ ./Processed_Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (817 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "  6%|▌         | 1/18 [02:27<41:52, 147.81s/it]\n",
      " 17%|█▋        | 3/18 [02:27<09:35, 38.36s/it] \n",
      " 28%|██▊       | 5/18 [02:28<04:02, 18.69s/it]\n",
      " 56%|█████▌    | 10/18 [02:28<00:52,  6.61s/it]\n",
      " 89%|████████▉ | 16/18 [02:28<00:06,  3.15s/it]\n",
      "100%|██████████| 18/18 [02:28<00:00,  8.25s/it]\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      " 10%|▉         | 2/21 [00:00<00:01, 10.49it/s]\n",
      " 48%|████▊     | 10/21 [00:00<00:00, 32.93it/s]\n",
      " 67%|██████▋   | 14/21 [00:00<00:00, 30.63it/s]\n",
      " 86%|████████▌ | 18/21 [00:00<00:00, 30.59it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 27.21it/s]\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      " 11%|█         | 2/19 [00:00<00:01, 12.89it/s]\n",
      " 26%|██▋       | 5/19 [00:00<00:00, 18.32it/s]\n",
      " 47%|████▋     | 9/19 [00:00<00:00, 22.18it/s]\n",
      " 63%|██████▎   | 12/19 [00:00<00:00, 19.47it/s]\n",
      " 74%|███████▎  | 14/19 [00:00<00:00, 18.48it/s]\n",
      " 95%|█████████▍| 18/19 [00:00<00:00, 21.56it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 21.33it/s]\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      " 29%|██▊       | 6/21 [00:00<00:00, 39.19it/s]\n",
      " 48%|████▊     | 10/21 [00:00<00:00, 26.78it/s]\n",
      " 71%|███████▏  | 15/21 [00:00<00:00, 30.87it/s]\n",
      " 95%|█████████▌| 20/21 [00:00<00:00, 31.36it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 32.74it/s]\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      " 47%|████▋     | 9/19 [00:00<00:00, 61.99it/s]\n",
      " 84%|████████▍ | 16/19 [00:00<00:00, 40.46it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 37.08it/s]\n",
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\n",
      " 16%|█▌        | 3/19 [00:00<00:00, 17.33it/s]\n",
      " 53%|█████▎    | 10/19 [00:00<00:00, 32.19it/s]\n",
      " 74%|███████▎  | 14/19 [00:00<00:00, 25.79it/s]\n",
      " 89%|████████▉ | 17/19 [00:00<00:00, 19.92it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 22.26it/s]\n",
      "\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\n",
      " 17%|█▋        | 3/18 [00:00<00:00, 17.80it/s]\n",
      " 33%|███▎      | 6/18 [00:00<00:00, 19.39it/s]\n",
      " 61%|██████    | 11/18 [00:00<00:00, 27.15it/s]\n",
      " 78%|███████▊  | 14/18 [00:00<00:00, 24.15it/s]\n",
      " 94%|█████████▍| 17/18 [00:00<00:00, 23.03it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 22.41it/s]\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      " 15%|█▌        | 3/20 [00:00<00:00, 19.16it/s]\n",
      " 45%|████▌     | 9/20 [00:00<00:00, 31.31it/s]\n",
      " 65%|██████▌   | 13/20 [00:00<00:00, 25.06it/s]\n",
      " 80%|████████  | 16/20 [00:00<00:00, 20.73it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 25.53it/s]\n",
      "\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\n",
      " 22%|██▏       | 4/18 [00:00<00:00, 23.66it/s]\n",
      " 50%|█████     | 9/18 [00:00<00:00, 30.42it/s]\n",
      " 72%|███████▏  | 13/18 [00:00<00:00, 30.71it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 34.27it/s]\n"
     ]
    }
   ],
   "source": [
    "!python ../Data_Preprocessing/Emotion_Label.py ./Processed_Data/ ./Processed_Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file=open(\"./Processed_Data/lakh_emotion.bin\",\"rb\")\n",
    "music_data = pickle.load(file) #保存list到文件\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['amazing', 'anger', 'angry', 'awe', 'emotional', 'excited', 'fear',\n",
       "       'happy', 'hopeful', 'hurt', 'joy', 'joyous', 'love', 'negative',\n",
       "       'positive', 'powerful', 'sad', 'sadness', 'strong', 'uneasy',\n",
       "       'upset'], dtype='<U9')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the labels\n",
    "np.unique([x[1] for x in music_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a5ONeVUU7MOU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selected the most common one word labels\n",
    "from tqdm import tqdm\n",
    "X = []\n",
    "y = []\n",
    "chosen_label = ['sadness', 'happy', 'love', 'joy', 'negative', 'positive', 'anger',\n",
    "       'awe', 'emotional', 'hopeful', 'angry', 'excited', 'fear', 'hurt',\n",
    "       'fearful']\n",
    "# for i in tqdm(range(1,10)):\n",
    "file=open(\"./Processed_Data/lakh_emotion.bin\",\"rb\")\n",
    "music_data = pickle.load(file) #保存list到文件\n",
    "file.close()\n",
    "for m in music_data:\n",
    "    if m[1] in chosen_label:\n",
    "        X.append(m[0])\n",
    "        if m[1] == 'anger':\n",
    "            m[1] = 'angry'\n",
    "        if m[1] == 'fearful':\n",
    "            m[1] = 'fear'\n",
    "        if m[1] == 'joy':\n",
    "            m[1] = 'happy'\n",
    "        y.append(m[1])\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "X_new = []\n",
    "y_new = []\n",
    "for l in np.unique(y):\n",
    "    n = (y==l).sum()\n",
    "    if n>=6:\n",
    "        X_new += list(X[y==l])\n",
    "        y_new += list(y[y==l])\n",
    "X = X_new\n",
    "y = y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform labels into one-hot variable\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "label_dict = {}\n",
    "for cl in le.classes_:\n",
    "    label_dict.update({cl:le.transform([cl])[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, hiddenSize, outChannels, dropoutRate, activate):\n",
    "        super().__init__()\n",
    "        self.outChannels = outChannels\n",
    "        self.activate = nn.Sigmoid() if activate == \"Sigmoid\" else nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(1, 24, (10,1))\n",
    "        self.pool = nn.MaxPool2d((2, 1))\n",
    "        self.conv2 = nn.Conv2d(24, 48, (10,1))\n",
    "        self.conv3 = nn.Conv2d(48, 96, (10,1))\n",
    "        self.conv4 = nn.Conv2d(96, 192, (10,1))\n",
    "        self.conv5 = nn.Conv2d(192, 384, (5,2))\n",
    "        self.conv6 = nn.Conv2d(384, 192, (5,2))\n",
    "        self.dense1 = nn.Linear(48384, hiddenSize)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "        self.dense2 = nn.Linear(hiddenSize, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activate(self.conv1(x)))\n",
    "        x = self.pool(self.activate(self.conv2(x)))\n",
    "        # print(x.shape)\n",
    "        x = self.dropout(self.pool(self.activate(self.conv3(x))))\n",
    "        x = self.pool(self.activate(self.conv4(x)))\n",
    "        x = self.pool(self.activate(self.conv5(x)))\n",
    "        x = self.pool(self.activate(self.conv6(x)))\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 48384)\n",
    "        x = self.dropout(self.activate(self.dense1(x)))\n",
    "        return self.dense2(x)\n",
    "\n",
    "# Number of neurons in the first fully-connected layer\n",
    "hiddenSize = 4096\n",
    "# Number of feature filters in second convolutional layer\n",
    "numFilters = 25\n",
    "# Dropout rate\n",
    "dropoutRate = 0.2\n",
    "# Activation function\n",
    "activation = \"ReLU\"\n",
    "# Learning rate\n",
    "learningRate = 0.005\n",
    "# Momentum for SGD optimizer\n",
    "momentum = 0.9\n",
    "# Number of training epochs\n",
    "numEpochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define Trian and Validation function\n",
    "\n",
    "from tqdm import tqdm\n",
    "def train_epoch(cnn,device,dataloader,loss_fn,optimizer):\n",
    "    cnn.train()\n",
    "    cnnRunningLoss = 0\n",
    "    total = 0\n",
    "    R2 = 0\n",
    "    cnnCorrect=0\n",
    "    total1=0\n",
    "    for i, (inputs, labels) in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to('cuda').reshape(-1,1,512,128)\n",
    "        \n",
    "        labels = labels.to('cuda')\n",
    "        # Forward propagation\n",
    "        cnnOutputs = cnn(inputs)\n",
    "        # print(cnnOutputs.shape)\n",
    "        l2_lambda = 0.001\n",
    "        l2_reg = torch.tensor(0.).to('cuda')\n",
    "        for param in cnn.parameters():\n",
    "            l2_reg += torch.norm(param)\n",
    "        # Backpropagation\n",
    "        cnnLoss = criterion(cnnOutputs, labels)+l2_reg*l2_lambda\n",
    "        cnnLoss.backward()\n",
    "        # Gradient update\n",
    "        optimizer.step()\n",
    "        total += 1\n",
    "        total1+=labels.size(0)\n",
    "        cnnRunningLoss += cnnLoss.item()\n",
    "        _, cnnPredicted = torch.max(cnnOutputs.data, 1)\n",
    "        cnnCorrect += (cnnPredicted == labels).sum().item()\n",
    "    return cnnRunningLoss/total, cnnCorrect/total1\n",
    "\n",
    "def valid_epoch(cnn,device,dataloader,loss_fn):\n",
    "    cnn.eval()\n",
    "    totalLoss = 0\n",
    "    total = 0\n",
    "    total1 = 0\n",
    "    R2 = 0\n",
    "    cnnLoss = 0\n",
    "    cnnCorrect=0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to('cuda').reshape(-1,1,512,128)\n",
    "        labels = labels.to('cuda')\n",
    "        cnnOutputs = cnn(inputs)\n",
    "        cnnLoss = loss_fn(cnnOutputs, labels)\n",
    "        _, cnnPredicted = torch.max(cnnOutputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        total1 +=1\n",
    "        totalLoss += cnnLoss.item()\n",
    "        cnnCorrect += (cnnPredicted == labels).sum().item()\n",
    "    accuracy = cnnCorrect / total * 100\n",
    "    cnn.train()\n",
    "    return totalLoss/total1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_18004\\2016658442.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  X_train_v = torch.as_tensor(X_train_v, dtype=torch.float) # an alternative to torch.from_numpy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train Loss: 0.748879685997963, Train_acc: 0.445, Test Loss: 0.6353520154953003, Test acc: 72.5\n",
      "Train Loss: 0.6721135824918747, Train_acc: 0.545, Test Loss: 0.5202437043190002, Test acc: 72.5\n",
      "Train Loss: 0.6057620868086815, Train_acc: 0.565, Test Loss: 0.40458256006240845, Test acc: 100.0\n",
      "Train Loss: 0.5238330662250519, Train_acc: 0.845, Test Loss: 0.32948991656303406, Test acc: 100.0\n",
      "Train Loss: 0.39969880878925323, Train_acc: 0.885, Test Loss: 0.12069686502218246, Test acc: 100.0\n",
      "Train Loss: 0.3175176605582237, Train_acc: 0.87, Test Loss: 0.06911692023277283, Test acc: 100.0\n",
      "Train Loss: 0.28870122879743576, Train_acc: 0.87, Test Loss: 0.07734281569719315, Test acc: 100.0\n",
      "Train Loss: 0.2672156095504761, Train_acc: 0.92, Test Loss: 0.05601201206445694, Test acc: 100.0\n",
      "Train Loss: 0.2213122472167015, Train_acc: 0.92, Test Loss: 0.050890274345874786, Test acc: 100.0\n",
      "Train Loss: 0.2031673826277256, Train_acc: 0.92, Test Loss: 0.04554833844304085, Test acc: 100.0\n",
      "Train Loss: 0.24048110842704773, Train_acc: 0.92, Test Loss: 0.04135897010564804, Test acc: 100.0\n",
      "Train Loss: 0.24649040400981903, Train_acc: 0.92, Test Loss: 0.04201293736696243, Test acc: 100.0\n",
      "Train Loss: 0.19847037643194199, Train_acc: 0.92, Test Loss: 0.04622845724225044, Test acc: 100.0\n",
      "Train Loss: 0.21141201257705688, Train_acc: 0.92, Test Loss: 0.049284808337688446, Test acc: 100.0\n",
      "Train Loss: 0.23834457993507385, Train_acc: 0.92, Test Loss: 0.052950888872146606, Test acc: 100.0\n",
      "Train Loss: 0.22352558374404907, Train_acc: 0.92, Test Loss: 0.054251573979854584, Test acc: 100.0\n",
      "Train Loss: 0.19929106533527374, Train_acc: 0.92, Test Loss: 0.054008714854717255, Test acc: 100.0\n",
      "Train Loss: 0.23618365451693535, Train_acc: 0.92, Test Loss: 0.052807606756687164, Test acc: 100.0\n",
      "Train Loss: 0.19751939922571182, Train_acc: 0.92, Test Loss: 0.051429737359285355, Test acc: 100.0\n",
      "Train Loss: 0.23660533875226974, Train_acc: 0.92, Test Loss: 0.04959992319345474, Test acc: 100.0\n",
      "Fold 2\n",
      "Train Loss: 0.7646641433238983, Train_acc: 0.505, Test Loss: 0.6594700813293457, Test acc: 50.0\n",
      "Train Loss: 0.7231566607952118, Train_acc: 0.595, Test Loss: 0.6028649806976318, Test acc: 50.0\n",
      "Train Loss: 0.6177197098731995, Train_acc: 0.595, Test Loss: 0.5389065146446228, Test acc: 50.0\n",
      "Train Loss: 0.47641217708587646, Train_acc: 0.775, Test Loss: 0.47952261567115784, Test acc: 87.5\n",
      "Train Loss: 0.35468487441539764, Train_acc: 0.895, Test Loss: 0.35999658703804016, Test acc: 87.5\n",
      "Train Loss: 0.2940466180443764, Train_acc: 0.895, Test Loss: 0.26989680528640747, Test acc: 87.5\n",
      "Train Loss: 0.24820952117443085, Train_acc: 0.895, Test Loss: 0.1337011307477951, Test acc: 100.0\n",
      "Train Loss: 0.2105472832918167, Train_acc: 0.945, Test Loss: 0.10170216858386993, Test acc: 100.0\n",
      "Train Loss: 0.19588593766093254, Train_acc: 0.945, Test Loss: 0.036846257746219635, Test acc: 100.0\n",
      "Train Loss: 0.19290939904749393, Train_acc: 0.945, Test Loss: 0.10435368120670319, Test acc: 87.5\n",
      "Train Loss: 0.19517988711595535, Train_acc: 0.945, Test Loss: 0.20094576478004456, Test acc: 87.5\n",
      "Train Loss: 0.19177472405135632, Train_acc: 0.945, Test Loss: 0.0977097898721695, Test acc: 87.5\n",
      "Train Loss: 0.18290015496313572, Train_acc: 0.945, Test Loss: 0.03946113586425781, Test acc: 100.0\n",
      "Train Loss: 0.1931312456727028, Train_acc: 0.945, Test Loss: 0.01521267555654049, Test acc: 100.0\n",
      "Train Loss: 0.2012905590236187, Train_acc: 0.945, Test Loss: 0.005251523572951555, Test acc: 100.0\n",
      "Train Loss: 0.2242637686431408, Train_acc: 0.945, Test Loss: 0.01573416404426098, Test acc: 100.0\n",
      "Train Loss: 0.22314798831939697, Train_acc: 0.945, Test Loss: 0.19965152442455292, Test acc: 87.5\n",
      "Train Loss: 0.21883821487426758, Train_acc: 0.945, Test Loss: 0.07269879430532455, Test acc: 100.0\n",
      "Train Loss: 0.22353797405958176, Train_acc: 0.945, Test Loss: 0.00684619415551424, Test acc: 100.0\n",
      "Train Loss: 0.1912169773131609, Train_acc: 0.945, Test Loss: 0.002529804827645421, Test acc: 100.0\n",
      "Fold 3\n",
      "Train Loss: 0.7679122686386108, Train_acc: 0.56, Test Loss: 0.6645895838737488, Test acc: 65.0\n",
      "Train Loss: 0.7124827653169632, Train_acc: 0.635, Test Loss: 0.5888436436653137, Test acc: 65.0\n",
      "Train Loss: 0.6795469373464584, Train_acc: 0.605, Test Loss: 0.44801074266433716, Test acc: 65.0\n",
      "Train Loss: 0.5186705216765404, Train_acc: 0.725, Test Loss: 0.5586013197898865, Test acc: 72.5\n",
      "Train Loss: 0.36951394006609917, Train_acc: 0.89, Test Loss: 1.1567022800445557, Test acc: 50.0\n",
      "Train Loss: 0.268552228808403, Train_acc: 0.89, Test Loss: 1.750444769859314, Test acc: 50.0\n",
      "Train Loss: 0.25996944680809975, Train_acc: 0.885, Test Loss: 5.476733207702637, Test acc: 15.0\n",
      "Train Loss: 0.22198907658457756, Train_acc: 0.93, Test Loss: 11.713224411010742, Test acc: 15.0\n",
      "Train Loss: 0.1995762325823307, Train_acc: 0.93, Test Loss: 19.707271575927734, Test acc: 15.0\n",
      "Train Loss: 0.1933642029762268, Train_acc: 0.93, Test Loss: 26.413555145263672, Test acc: 15.0\n",
      "Train Loss: 0.19155563227832317, Train_acc: 0.93, Test Loss: 28.03236961364746, Test acc: 15.0\n",
      "Train Loss: 0.2624865621328354, Train_acc: 0.93, Test Loss: 30.58209228515625, Test acc: 15.0\n",
      "Train Loss: 0.21283793076872826, Train_acc: 0.925, Test Loss: 25.24816131591797, Test acc: 50.0\n",
      "Train Loss: 0.20663443580269814, Train_acc: 0.89, Test Loss: 15.890096664428711, Test acc: 50.0\n",
      "Train Loss: 0.19981421530246735, Train_acc: 0.89, Test Loss: 6.739170074462891, Test acc: 50.0\n",
      "Train Loss: 0.1991400420665741, Train_acc: 0.895, Test Loss: 1.7738211154937744, Test acc: 72.5\n",
      "Train Loss: 0.23186452314257622, Train_acc: 0.93, Test Loss: 0.8059674501419067, Test acc: 37.5\n",
      "Train Loss: 0.2312600389122963, Train_acc: 0.93, Test Loss: 0.994306206703186, Test acc: 37.5\n",
      "Train Loss: 0.21532445028424263, Train_acc: 0.93, Test Loss: 2.749732732772827, Test acc: 15.0\n",
      "Train Loss: 0.17456096597015858, Train_acc: 0.93, Test Loss: 6.444174289703369, Test acc: 15.0\n",
      "Fold 4\n",
      "Train Loss: 0.7581981122493744, Train_acc: 0.545, Test Loss: 0.6954239010810852, Test acc: 50.0\n",
      "Train Loss: 0.68504798412323, Train_acc: 0.55, Test Loss: 0.7504880428314209, Test acc: 50.0\n",
      "Train Loss: 0.6583614945411682, Train_acc: 0.55, Test Loss: 0.9025439023971558, Test acc: 50.0\n",
      "Train Loss: 0.49468066543340683, Train_acc: 0.75, Test Loss: 0.6377147436141968, Test acc: 45.0\n",
      "Train Loss: 0.3662695437669754, Train_acc: 0.97, Test Loss: 0.533194363117218, Test acc: 45.0\n",
      "Train Loss: 0.2560150772333145, Train_acc: 0.985, Test Loss: 0.3359162211418152, Test acc: 72.5\n",
      "Train Loss: 0.16009346395730972, Train_acc: 1.0, Test Loss: 0.5485621690750122, Test acc: 72.5\n",
      "Train Loss: 0.1123258713632822, Train_acc: 1.0, Test Loss: 0.6685799360275269, Test acc: 72.5\n",
      "Train Loss: 0.0863640084862709, Train_acc: 1.0, Test Loss: 1.1324679851531982, Test acc: 72.5\n",
      "Train Loss: 0.7094192355871201, Train_acc: 0.98, Test Loss: 10.190049171447754, Test acc: 45.0\n",
      "Train Loss: 0.1076741348952055, Train_acc: 0.99, Test Loss: 6.170848369598389, Test acc: 45.0\n",
      "Train Loss: 0.12279727309942245, Train_acc: 0.995, Test Loss: 5.467813014984131, Test acc: 35.0\n",
      "Train Loss: 0.14006321504712105, Train_acc: 1.0, Test Loss: 3.3247578144073486, Test acc: 45.0\n",
      "Train Loss: 0.11863147839903831, Train_acc: 1.0, Test Loss: 4.195789337158203, Test acc: 45.0\n",
      "Train Loss: 0.09439055249094963, Train_acc: 1.0, Test Loss: 6.167839527130127, Test acc: 45.0\n",
      "Train Loss: 0.08702627383172512, Train_acc: 1.0, Test Loss: 8.076859474182129, Test acc: 45.0\n",
      "Train Loss: 0.08430290035903454, Train_acc: 1.0, Test Loss: 9.57746410369873, Test acc: 45.0\n",
      "Train Loss: 0.08330645598471165, Train_acc: 1.0, Test Loss: 10.600340843200684, Test acc: 45.0\n",
      "Train Loss: 0.08133799955248833, Train_acc: 1.0, Test Loss: 11.234331130981445, Test acc: 45.0\n",
      "Train Loss: 0.08074640668928623, Train_acc: 1.0, Test Loss: 11.651278495788574, Test acc: 45.0\n",
      "Fold 5\n",
      "Train Loss: 0.7685447335243225, Train_acc: 0.52, Test Loss: 0.7083697319030762, Test acc: 50.0\n",
      "Train Loss: 0.644517108798027, Train_acc: 0.665, Test Loss: 0.7477266192436218, Test acc: 50.0\n",
      "Train Loss: 0.5396017953753471, Train_acc: 0.71, Test Loss: 0.8329593539237976, Test acc: 50.0\n",
      "Train Loss: 0.4627627953886986, Train_acc: 0.75, Test Loss: 0.6584660410881042, Test acc: 22.5\n",
      "Train Loss: 0.37178266793489456, Train_acc: 0.945, Test Loss: 0.562926173210144, Test acc: 50.0\n",
      "Train Loss: 0.2706666961312294, Train_acc: 0.945, Test Loss: 0.5958688855171204, Test acc: 72.5\n",
      "Train Loss: 0.25917160511016846, Train_acc: 0.945, Test Loss: 0.6290732622146606, Test acc: 72.5\n",
      "Train Loss: 0.2028336487710476, Train_acc: 0.945, Test Loss: 0.5567255616188049, Test acc: 72.5\n",
      "Train Loss: 0.1910359151661396, Train_acc: 0.945, Test Loss: 0.4650820195674896, Test acc: 100.0\n",
      "Train Loss: 0.3276148773729801, Train_acc: 0.945, Test Loss: 0.5173047780990601, Test acc: 72.5\n",
      "Train Loss: 0.22056889161467552, Train_acc: 0.945, Test Loss: 1.0267521142959595, Test acc: 72.5\n",
      "Train Loss: 0.24485841393470764, Train_acc: 0.945, Test Loss: 1.0446029901504517, Test acc: 72.5\n",
      "Train Loss: 0.24340801313519478, Train_acc: 0.945, Test Loss: 0.9163112640380859, Test acc: 72.5\n",
      "Train Loss: 0.22652189433574677, Train_acc: 0.945, Test Loss: 0.7839770317077637, Test acc: 72.5\n",
      "Train Loss: 0.22445452585816383, Train_acc: 0.945, Test Loss: 0.6648083925247192, Test acc: 72.5\n",
      "Train Loss: 0.245174091309309, Train_acc: 0.945, Test Loss: 0.589013934135437, Test acc: 72.5\n",
      "Train Loss: 0.2203243337571621, Train_acc: 0.945, Test Loss: 0.5648956894874573, Test acc: 72.5\n",
      "Train Loss: 0.21080918610095978, Train_acc: 0.945, Test Loss: 0.560880184173584, Test acc: 72.5\n",
      "Train Loss: 0.19023923948407173, Train_acc: 0.945, Test Loss: 0.5906093716621399, Test acc: 72.5\n",
      "Train Loss: 0.2320932112634182, Train_acc: 0.945, Test Loss: 0.5984527468681335, Test acc: 72.5\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "X_train_v, X_test, y_train_v, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # First Split the dataset and use the training set for 5-fold validation\n",
    "\n",
    "X_train_v = torch.as_tensor(X_train_v, dtype=torch.float) # an alternative to torch.from_numpy\n",
    "y_train_v = torch.as_tensor(y_train_v, dtype=torch.float)\n",
    "X_test = torch.as_tensor(X_test, dtype=torch.float)\n",
    "y_test = torch.as_tensor(y_test, dtype=torch.float)\n",
    "splits=StratifiedKFold(n_splits=5,shuffle=True,random_state=99)\n",
    "train_dataset = TensorDataset(X_train_v, y_train_v)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "batch_size=128\n",
    "models = []\n",
    "history = {'fold':[], 'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(X_train_v,y_train_v)):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    t = pd.DataFrame(y_train_v[train_idx], columns = ['class']).reset_index()\n",
    "    idx = []\n",
    "    # Resample to avoid unbalanced dataset\n",
    "    y_train = []\n",
    "    for i in range(0,num_labels):\n",
    "      idx += list(t[t['class']==i].sample(100,replace=True)['index'])\n",
    "      y_train += [i]*100\n",
    "    X_train = X_train_v[train_idx][idx]\n",
    "\n",
    "    t = pd.DataFrame(y_train_v[val_idx], columns = ['class']).reset_index()\n",
    "    idx = []\n",
    "    y_val = []\n",
    "    for i in range(0,num_labels):\n",
    "      # print(i)\n",
    "      if i in t['class'].values:\n",
    "          idx += list(t[t['class']==i].sample(20,replace=True)['index'])\n",
    "          y_val += [i]*20\n",
    "    X_val =  X_train_v[val_idx][idx]\n",
    "\n",
    "    X_train = torch.as_tensor(X_train, dtype=torch.float) # an alternative to torch.from_numpy\n",
    "    y_train = torch.as_tensor(y_train, dtype=torch.float).type(torch.LongTensor)\n",
    "    X_val = torch.as_tensor(X_val, dtype=torch.float)\n",
    "    y_val = torch.as_tensor(y_val, dtype=torch.float).type(torch.LongTensor)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    model = CNNModel(hiddenSize, numFilters, dropoutRate, activation).to('cuda')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(list(model.parameters()), lr=learningRate, momentum=momentum)\n",
    "    # optimizer = torch.optim.Adam(list(model.parameters()), lr=learningRate)\n",
    "    best_test = -np.inf\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(numEpochs):\n",
    "        train_loss, train_R2=train_epoch(model,device,train_dataloader,criterion,optimizer)\n",
    "        test_loss, test_R2=valid_epoch(model,device,test_dataloader,criterion)\n",
    "        history['fold'].append(fold)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_R2)\n",
    "        history['test_acc'].append(test_R2)   \n",
    "        if test_R2>best_test:\n",
    "            test_test = test_R2\n",
    "            best_model = model\n",
    "        print('Train Loss: {}, Train_acc: {}, Test Loss: {}, Test acc: {}'.format(train_loss, train_R2, test_loss, test_R2))\n",
    "    models.append([best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Caculate the average accuray\n",
    "# X_train_v, X_test, y_train_v, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "t = pd.DataFrame(y_test, columns = ['class']).reset_index()\n",
    "idx = []\n",
    "y_test = []\n",
    "for i in range(0,num_labels):\n",
    "    if i in t['class'].values:\n",
    "        idx += list(t[t['class']==i].sample(20,replace=True)['index'])\n",
    "        y_test += [i]*20\n",
    "X_test = np.array(X_test)\n",
    "X_test =  X_test[idx]\n",
    "X_test = torch.as_tensor(X_test, dtype=torch.float)\n",
    "y_test = torch.as_tensor(y_test, dtype=torch.float).type(torch.LongTensor)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "acc=[]\n",
    "for best_model in models:\n",
    "    acc.append(valid_epoch(best_model[0],device,test_dataloader,criterion)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the accuray of the ensemble model\n",
    "outputs = []    \n",
    "for model in models:\n",
    "  model[0].eval()\n",
    "  model_output = []\n",
    "  label_= []\n",
    "  model[0].to('cpu')\n",
    "  for inputs, labels in test_dataloader:\n",
    "      inputs = inputs.to('cpu').reshape(-1,1,512,128)\n",
    "      labels = labels.to('cpu')\n",
    "      cnnOutputs = model[0](inputs)\n",
    "      model_output.append(cnnOutputs)\n",
    "      label_.append(labels)\n",
    "  outputs.append(torch.vstack(model_output))\n",
    "shape = outputs[0].shape\n",
    "cnnresult = torch.vstack(outputs).reshape(-1,shape[0],shape[1]).mean(axis=0)\n",
    "labels = np.hstack(label_)\n",
    "_, cnnPredicted = torch.max(cnnresult.data, 1)\n",
    "cnnCorrect = (cnnPredicted.detach().numpy() == labels).sum().item()\n",
    "cnnCorrect/len(labels)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Draw the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confu_m = confusion_matrix(labels, cnnPredicted)\n",
    "import seaborn as sns\n",
    "sns.heatmap(confu_m,square=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "for cl in le.classes_:\n",
    "    label_dict.update({cl:le.transform([cl])[0]})\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "scores = pd.DataFrame(precision_recall_fscore_support(cnnPredicted, labels, labels = list(range(12))),index=['precision','recall','f1','support'], columns = list(label_dict)[:12])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
