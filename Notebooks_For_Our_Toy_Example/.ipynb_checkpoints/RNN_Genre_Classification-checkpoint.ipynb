{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Train the RNN Genre Classification Model#\n",
        "# Last editted by Pu Zeng, 18/10/2023 #"
      ],
      "metadata": {
        "id": "UMzLgypdrY3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "# import mido\n",
        "import string\n",
        "import numpy as np\n",
        "# from utilis import get_pianoroll_data\n",
        "import pickle\n",
        "from sklearn import preprocessing\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #Define device\n",
        "file_dir = \"/content/drive/MyDrive/Milestone 2/\" #Please change"
      ],
      "metadata": {
        "id": "u8eSS9tg62ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Processing\n",
        "\n",
        "# from tqdm import tqdm\n",
        "# X = []\n",
        "# y = []\n",
        "# for i in tqdm(range(1,10)):\n",
        "#     file=open(file_dir+\"music_data\"+str(i)+\".bin\",\"rb\")\n",
        "#     music_data = pickle.load(file) #保存list到文件\n",
        "#     file.close()\n",
        "#     for m in music_data:\n",
        "#         if m[2].shape[0]>=1012:\n",
        "#             X.append(m[2][500:1012,:])\n",
        "#             y.append(m[0])\n",
        "# X = np.array(X)"
      ],
      "metadata": {
        "id": "a5ONeVUU7MOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save Processed Data\n",
        "\n",
        "# import pickle\n",
        "# file = open('data','wb')\n",
        "# pickle.dump([X,y], file)\n",
        "# file.close()"
      ],
      "metadata": {
        "id": "ZpXcoqzIn0T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Processed Data\n",
        "\n",
        "import pickle\n",
        "file = open('data','rb')\n",
        "X, y = pickle.load(file)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "Q8vBuLAUn_3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform labels into one-hot variables\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "label_dict = {}\n",
        "for cl in le.classes_:\n",
        "    label_dict.update({cl:le.transform([cl])[0]})"
      ],
      "metadata": {
        "id": "twiggB0BmI2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN Model\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W = nn.Linear(hidden_size, hidden_size)\n",
        "        self.v = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x => (batch_size, seq_len, hidden_size)\n",
        "        scores = self.v(torch.tanh(self.W(x))) # (batch_size, seq_len, 1)\n",
        "        attention_weights = F.softmax(scores, dim=1)\n",
        "        context_vector = attention_weights * x\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        # self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, 2, dropout=0.3, batch_first=True, bidirectional=True)\n",
        "        self.attention = Attention(hidden_size * 2)\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        context_vector, attention_weights = self.attention(lstm_out)\n",
        "        output = self.fc(context_vector)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# from tqdm import tqdm\n",
        "# cnn = TextClassifier(128, embedding_dim=128, hidden_size=1024, num_classes=17).to('cuda')\n",
        "\n",
        "hiddenSize = 300\n",
        "# Number of feature filters in second convolutional layer\n",
        "numFilters = 25\n",
        "# Dropout rate\n",
        "dropoutRate = 0.3\n",
        "# Activation function\n",
        "activation = \"ReLU\"\n",
        "# Learning rate\n",
        "learningRate = 0.01\n",
        "# Momentum for SGD optimizer\n",
        "momentum = 0.9\n",
        "# Number of training epochs\n",
        "numEpochs = 30"
      ],
      "metadata": {
        "id": "p80pW8l5HiMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Validation function\n",
        "\n",
        "from tqdm import tqdm\n",
        "def train_epoch(cnn,device,dataloader,loss_fn,optimizer):\n",
        "    cnn.train()\n",
        "    cnnRunningLoss = 0\n",
        "    total = 0\n",
        "    R2 = 0\n",
        "    cnnCorrect=0\n",
        "    total1=0\n",
        "    for i, (inputs, labels) in enumerate(dataloader, 0):\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.to(device).reshape(-1,512,128)\n",
        "\n",
        "        labels = labels.to(device)\n",
        "        # Forward propagation\n",
        "        cnnOutputs = cnn(inputs)\n",
        "        # print(cnnOutputs.shape)\n",
        "        l2_lambda = 0.005\n",
        "        l2_reg = torch.tensor(0.).to(device)\n",
        "        for param in cnn.parameters():\n",
        "            l2_reg += torch.norm(param)\n",
        "        # Backpropagation\n",
        "        cnnLoss = criterion(cnnOutputs, labels)+l2_reg*l2_lambda\n",
        "        cnnLoss.backward()\n",
        "        # Gradient update\n",
        "        optimizer.step()\n",
        "        total += 1\n",
        "        total1+=labels.size(0)\n",
        "        cnnRunningLoss += cnnLoss.item()\n",
        "        _, cnnPredicted = torch.max(cnnOutputs.data, 1)\n",
        "        cnnCorrect += (cnnPredicted == labels).sum().item()\n",
        "    return cnnRunningLoss/total, cnnCorrect/total1\n",
        "\n",
        "def valid_epoch(cnn,device,dataloader,loss_fn):\n",
        "    cnn.eval()\n",
        "    totalLoss = 0\n",
        "    total = 0\n",
        "    total1 = 0\n",
        "    R2 = 0\n",
        "    cnnLoss = 0\n",
        "    cnnCorrect=0\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device).reshape(-1,512,128)\n",
        "        labels = labels.to(device)\n",
        "        cnnOutputs = cnn(inputs)\n",
        "        cnnLoss = criterion(cnnOutputs, labels)\n",
        "        _, cnnPredicted = torch.max(cnnOutputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        total1 +=1\n",
        "        totalLoss += cnnLoss.item()\n",
        "        cnnCorrect += (cnnPredicted == labels).sum().item()\n",
        "    accuracy = cnnCorrect / total\n",
        "    cnn.train()\n",
        "    return totalLoss/total1, accuracy"
      ],
      "metadata": {
        "id": "rds7kBErIyiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "X_train_v, X_test, y_train_v, y_test = train_test_split(X, y, test_size=0.2, random_state=99, stratify=y) #First split, so there is no data leakage problems\n",
        "\n",
        "# Transform data into DataSet\n",
        "X_train_v = torch.as_tensor(X_train_v, dtype=torch.float) # an alternative to torch.from_numpy\n",
        "y_train_v = torch.as_tensor(y_train_v, dtype=torch.float)\n",
        "X_test = torch.as_tensor(X_test, dtype=torch.float)\n",
        "y_test = torch.as_tensor(y_test, dtype=torch.float)\n",
        "splits=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
        "train_dataset = TensorDataset(X_train_v, y_train_v)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "batch_size=128\n",
        "models = []\n",
        "history = {'fold':[], 'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
        "\n",
        "#K-Fold validation\n",
        "for fold, (train_idx,val_idx) in enumerate(splits.split(X_train_v,y_train_v)):\n",
        "    print('Fold {}'.format(fold + 1))\n",
        "    t = pd.DataFrame(y_train_v[train_idx], columns = ['class']).reset_index()\n",
        "    idx = []\n",
        "    y_train = []\n",
        "    #Resampling to solve the unbalanced problem\n",
        "    for i in range(0,18):\n",
        "        if i!=label_dict['Unknown']:\n",
        "          idx += list(t[t['class']==i].sample(500,replace=True)['index'])\n",
        "          y_train += [i]*500\n",
        "    X_train = X_train_v[train_idx][idx]\n",
        "\n",
        "    t = pd.DataFrame(y_train_v[val_idx], columns = ['class']).reset_index()\n",
        "    idx = []\n",
        "    y_val = []\n",
        "    for i in range(0,18):\n",
        "        if i!=label_dict['Unknown']:\n",
        "          idx += list(t[t['class']==i].sample(20,replace=True)['index'])\n",
        "          y_val += [i]*20\n",
        "    X_val =  X_train_v[val_idx][idx]\n",
        "\n",
        "    X_train = torch.as_tensor(X_train, dtype=torch.float) # an alternative to torch.from_numpy\n",
        "    y_train = torch.as_tensor(y_train, dtype=torch.float).type(torch.LongTensor)\n",
        "    X_val = torch.as_tensor(X_val, dtype=torch.float)\n",
        "    y_val = torch.as_tensor(y_val, dtype=torch.float).type(torch.LongTensor)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "    from torch.utils.data import Dataset, DataLoader\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "    model = TextClassifier(128, embedding_dim=128, hidden_size=1024, num_classes=18).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(list(model.parameters()), lr=learningRate, momentum=momentum)\n",
        "    # optimizer = torch.optim.Adam(list(model.parameters()), lr=learningRate)\n",
        "    best_test = -np.inf\n",
        "    best_model = None\n",
        "\n",
        "    #Train the model\n",
        "    for epoch in range(numEpochs):\n",
        "        train_loss, train_R2=train_epoch(model,device,train_dataloader,criterion,optimizer)\n",
        "        test_loss, test_R2=valid_epoch(model,device,test_dataloader,criterion)\n",
        "        history['fold'].append(fold)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['train_acc'].append(train_R2)\n",
        "        history['test_acc'].append(test_R2)\n",
        "        if test_R2>best_test:\n",
        "            test_test = test_R2\n",
        "            best_model = model\n",
        "        print('Train Loss: {}, Train_acc: {}, Test Loss: {}, Test acc: {}'.format(train_loss, train_R2, test_loss, test_R2))\n",
        "    models.append([best_model])"
      ],
      "metadata": {
        "id": "vrgmoWX3jTp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22367de-e15a-431a-af98-9453c9afa5c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n",
            "Train Loss: 4.384745462616878, Train_acc: 0.18905882352941175, Test Loss: 2.7572216987609863, Test acc: 0.19117647058823528\n",
            "Train Loss: 4.1432179230362625, Train_acc: 0.2645882352941176, Test Loss: 2.611772139867147, Test acc: 0.19411764705882353\n",
            "Train Loss: 3.8598356318117966, Train_acc: 0.336, Test Loss: 2.5226550102233887, Test acc: 0.21764705882352942\n",
            "Train Loss: 3.513877708520462, Train_acc: 0.43329411764705883, Test Loss: 2.464976151784261, Test acc: 0.2911764705882353\n",
            "Train Loss: 3.062562148962448, Train_acc: 0.5798823529411765, Test Loss: 2.4917096296946206, Test acc: 0.38529411764705884\n",
            "Train Loss: 2.6379426166192808, Train_acc: 0.7058823529411765, Test Loss: 2.513784170150757, Test acc: 0.4411764705882353\n",
            "Train Loss: 2.399580749113168, Train_acc: 0.7785882352941177, Test Loss: 2.5594301223754883, Test acc: 0.43823529411764706\n",
            "Train Loss: 2.268087643295971, Train_acc: 0.8136470588235294, Test Loss: 2.6234165032704673, Test acc: 0.4676470588235294\n",
            "Train Loss: 2.1113619217232094, Train_acc: 0.8549411764705882, Test Loss: 2.8133832613627114, Test acc: 0.5088235294117647\n",
            "Train Loss: 2.0313967501939234, Train_acc: 0.8781176470588236, Test Loss: 2.7790902058283486, Test acc: 0.4970588235294118\n",
            "Train Loss: 2.0758559294600984, Train_acc: 0.8671764705882353, Test Loss: 3.0224323670069375, Test acc: 0.4852941176470588\n",
            "Train Loss: 1.9864935572467632, Train_acc: 0.8917647058823529, Test Loss: 2.815245270729065, Test acc: 0.5088235294117647\n",
            "Train Loss: 1.897160291671753, Train_acc: 0.9172941176470588, Test Loss: 3.0988500118255615, Test acc: 0.5\n",
            "Train Loss: 1.8896646730935396, Train_acc: 0.9190588235294118, Test Loss: 3.2231333255767822, Test acc: 0.4852941176470588\n",
            "Train Loss: 1.8574348741502904, Train_acc: 0.9264705882352942, Test Loss: 3.0515060424804688, Test acc: 0.5176470588235295\n",
            "Train Loss: 1.898787400615749, Train_acc: 0.9143529411764706, Test Loss: 3.2349302768707275, Test acc: 0.5058823529411764\n",
            "Train Loss: 1.8085182584933381, Train_acc: 0.941764705882353, Test Loss: 3.4258206685384116, Test acc: 0.5235294117647059\n",
            "Train Loss: 1.7678601777375633, Train_acc: 0.9529411764705882, Test Loss: 3.4875579675038657, Test acc: 0.5176470588235295\n",
            "Train Loss: 1.7433903110561086, Train_acc: 0.9589411764705882, Test Loss: 3.751667102177938, Test acc: 0.5117647058823529\n",
            "Train Loss: 1.7321382743209155, Train_acc: 0.9614117647058823, Test Loss: 3.8770928382873535, Test acc: 0.5176470588235295\n",
            "Train Loss: 1.7231215114024148, Train_acc: 0.9631764705882353, Test Loss: 3.9267897605895996, Test acc: 0.5205882352941177\n",
            "Train Loss: 1.7175660489210443, Train_acc: 0.964, Test Loss: 3.984421173731486, Test acc: 0.5117647058823529\n",
            "Train Loss: 1.7122572286805111, Train_acc: 0.9645882352941176, Test Loss: 4.003239711125691, Test acc: 0.5117647058823529\n",
            "Train Loss: 1.7082376996083046, Train_acc: 0.9645882352941176, Test Loss: 4.0155205726623535, Test acc: 0.5117647058823529\n",
            "Train Loss: 1.7054709996750106, Train_acc: 0.9641176470588235, Test Loss: 4.082288424173991, Test acc: 0.5058823529411764\n",
            "Train Loss: 1.7005852496446068, Train_acc: 0.9648235294117647, Test Loss: 4.083216031392415, Test acc: 0.5088235294117647\n",
            "Train Loss: 1.6972106463873564, Train_acc: 0.9644705882352941, Test Loss: 4.13262399037679, Test acc: 0.5088235294117647\n",
            "Train Loss: 1.6960105326638293, Train_acc: 0.964, Test Loss: 4.157165288925171, Test acc: 0.5058823529411764\n",
            "Train Loss: 1.6916204068198133, Train_acc: 0.9645882352941176, Test Loss: 4.17633318901062, Test acc: 0.5117647058823529\n",
            "Train Loss: 1.6889166867555077, Train_acc: 0.9643529411764706, Test Loss: 4.170973539352417, Test acc: 0.5117647058823529\n",
            "Train Loss: 1.6858870805199466, Train_acc: 0.9647058823529412, Test Loss: 4.161981503168742, Test acc: 0.5147058823529411\n",
            "Train Loss: 1.6807534570124611, Train_acc: 0.9647058823529412, Test Loss: 4.197871685028076, Test acc: 0.5176470588235295\n",
            "Train Loss: 1.6788701957731105, Train_acc: 0.9645882352941176, Test Loss: 4.2338783740997314, Test acc: 0.5117647058823529\n",
            "Train Loss: 1.6750117664906516, Train_acc: 0.9651764705882353, Test Loss: 4.217842976252238, Test acc: 0.5117647058823529\n",
            "Train Loss: 1.674140004969355, Train_acc: 0.9649411764705882, Test Loss: 4.200814326604207, Test acc: 0.5088235294117647\n",
            "Train Loss: 1.6699377316147535, Train_acc: 0.9648235294117647, Test Loss: 4.238792975743611, Test acc: 0.5117647058823529\n",
            "Train Loss: 1.6664822030423292, Train_acc: 0.9652941176470589, Test Loss: 4.248221397399902, Test acc: 0.5117647058823529\n",
            "Train Loss: 1.6649195222712274, Train_acc: 0.9649411764705882, Test Loss: 4.249115626017253, Test acc: 0.5176470588235295\n",
            "Train Loss: 1.6620848748221326, Train_acc: 0.9649411764705882, Test Loss: 4.270260254542033, Test acc: 0.5235294117647059\n",
            "Train Loss: 1.6599851953449534, Train_acc: 0.9651764705882353, Test Loss: 4.31275733311971, Test acc: 0.5088235294117647\n",
            "Train Loss: 1.6563785698876452, Train_acc: 0.9650588235294117, Test Loss: 4.313475608825684, Test acc: 0.5117647058823529\n",
            "Train Loss: 1.654257671156926, Train_acc: 0.9650588235294117, Test Loss: 4.316020727157593, Test acc: 0.5147058823529411\n",
            "Train Loss: 1.6517341688497742, Train_acc: 0.9652941176470589, Test Loss: 4.296494801839192, Test acc: 0.5117647058823529\n",
            "Train Loss: 1.6512084754545298, Train_acc: 0.9654117647058823, Test Loss: 4.327262004216512, Test acc: 0.5205882352941177\n",
            "Train Loss: 1.6487574203690487, Train_acc: 0.9656470588235294, Test Loss: 4.399367729822795, Test acc: 0.5147058823529411\n",
            "Train Loss: 1.6465285464898864, Train_acc: 0.9658823529411765, Test Loss: 4.372649749120076, Test acc: 0.5235294117647059\n",
            "Train Loss: 1.6438868152561472, Train_acc: 0.9650588235294117, Test Loss: 4.353811502456665, Test acc: 0.5235294117647059\n",
            "Train Loss: 1.6428734686837267, Train_acc: 0.9658823529411765, Test Loss: 4.427174170811971, Test acc: 0.5205882352941177\n",
            "Train Loss: 1.6406245018119243, Train_acc: 0.9661176470588235, Test Loss: 4.376121123631795, Test acc: 0.5294117647058824\n",
            "Train Loss: 1.6371747255325317, Train_acc: 0.9665882352941176, Test Loss: 4.397880872090657, Test acc: 0.5264705882352941\n",
            "Fold 2\n",
            "Train Loss: 4.384893104211608, Train_acc: 0.18964705882352942, Test Loss: 2.7535650730133057, Test acc: 0.15294117647058825\n",
            "Train Loss: 4.14259744402188, Train_acc: 0.2727058823529412, Test Loss: 2.6209470431009927, Test acc: 0.17941176470588235\n",
            "Train Loss: 3.863677576406678, Train_acc: 0.34894117647058825, Test Loss: 2.4918367862701416, Test acc: 0.29705882352941176\n",
            "Train Loss: 3.487935934493791, Train_acc: 0.446, Test Loss: 2.4118007024129233, Test acc: 0.3411764705882353\n",
            "Train Loss: 2.982637376927618, Train_acc: 0.6058823529411764, Test Loss: 2.2850748697916665, Test acc: 0.4323529411764706\n",
            "Train Loss: 2.636870992717458, Train_acc: 0.7045882352941176, Test Loss: 2.3573261896769204, Test acc: 0.43529411764705883\n",
            "Train Loss: 2.4102308607813137, Train_acc: 0.7725882352941177, Test Loss: 2.311017950375875, Test acc: 0.49411764705882355\n",
            "Train Loss: 2.2825428016150178, Train_acc: 0.8121176470588235, Test Loss: 2.3115400473276773, Test acc: 0.5088235294117647\n",
            "Train Loss: 2.145918061484152, Train_acc: 0.8483529411764706, Test Loss: 2.51248562335968, Test acc: 0.5147058823529411\n",
            "Train Loss: 2.015708272136859, Train_acc: 0.8827058823529412, Test Loss: 2.467373331387838, Test acc: 0.5470588235294118\n",
            "Train Loss: 2.0527083251013685, Train_acc: 0.8747058823529412, Test Loss: 2.7100937366485596, Test acc: 0.5058823529411764\n",
            "Train Loss: 2.021171295820777, Train_acc: 0.8858823529411765, Test Loss: 2.455640355745951, Test acc: 0.5205882352941177\n",
            "Train Loss: 1.9127641507049105, Train_acc: 0.9152941176470588, Test Loss: 2.588136593500773, Test acc: 0.55\n",
            "Train Loss: 1.8478077952541523, Train_acc: 0.9329411764705883, Test Loss: 2.805262565612793, Test acc: 0.538235294117647\n",
            "Train Loss: 1.7819555481868004, Train_acc: 0.9483529411764706, Test Loss: 3.020095626513163, Test acc: 0.5588235294117647\n",
            "Train Loss: 1.762005998127496, Train_acc: 0.959764705882353, Test Loss: 3.0786092281341553, Test acc: 0.5647058823529412\n",
            "Train Loss: 1.7500458333029676, Train_acc: 0.962, Test Loss: 3.2059290409088135, Test acc: 0.5617647058823529\n",
            "Train Loss: 1.7402323466628344, Train_acc: 0.9641176470588235, Test Loss: 3.269819974899292, Test acc: 0.5735294117647058\n",
            "Train Loss: 1.7862821639473758, Train_acc: 0.9532941176470588, Test Loss: 2.9376478592554727, Test acc: 0.55\n",
            "Train Loss: 1.7827453577696388, Train_acc: 0.9538823529411765, Test Loss: 2.9744426806767783, Test acc: 0.5588235294117647\n",
            "Train Loss: 1.7394059309318883, Train_acc: 0.9612941176470589, Test Loss: 3.235013961791992, Test acc: 0.5764705882352941\n",
            "Train Loss: 1.7175079299442804, Train_acc: 0.9695294117647059, Test Loss: 3.262464920679728, Test acc: 0.5735294117647058\n",
            "Train Loss: 1.7064925503374926, Train_acc: 0.9715294117647059, Test Loss: 3.365314324696859, Test acc: 0.5529411764705883\n",
            "Train Loss: 1.6992816462445615, Train_acc: 0.9704705882352941, Test Loss: 3.34110426902771, Test acc: 0.5588235294117647\n",
            "Train Loss: 1.6931909749756997, Train_acc: 0.9724705882352941, Test Loss: 3.5100653171539307, Test acc: 0.5676470588235294\n",
            "Train Loss: 1.6877304600245917, Train_acc: 0.9728235294117648, Test Loss: 3.6133739948272705, Test acc: 0.5676470588235294\n",
            "Train Loss: 1.6833843544347962, Train_acc: 0.9725882352941176, Test Loss: 3.626101016998291, Test acc: 0.5705882352941176\n",
            "Train Loss: 1.67925947459776, Train_acc: 0.9729411764705882, Test Loss: 3.581603447596232, Test acc: 0.5676470588235294\n",
            "Train Loss: 1.6738045553662884, Train_acc: 0.9728235294117648, Test Loss: 3.6576587359110513, Test acc: 0.5676470588235294\n",
            "Train Loss: 1.6702465068048507, Train_acc: 0.9735294117647059, Test Loss: 3.7052993774414062, Test acc: 0.5705882352941176\n",
            "Train Loss: 1.6675204714732383, Train_acc: 0.9736470588235294, Test Loss: 3.7538259824117026, Test acc: 0.5705882352941176\n",
            "Train Loss: 1.6637407808161493, Train_acc: 0.9734117647058823, Test Loss: 3.702129681905111, Test acc: 0.5705882352941176\n",
            "Train Loss: 1.6615586334200048, Train_acc: 0.9731764705882353, Test Loss: 3.7939974466959634, Test acc: 0.5735294117647058\n",
            "Train Loss: 1.6571326914118296, Train_acc: 0.9735294117647059, Test Loss: 3.807382106781006, Test acc: 0.5735294117647058\n",
            "Train Loss: 1.6563136310719733, Train_acc: 0.973764705882353, Test Loss: 3.873286406199137, Test acc: 0.5735294117647058\n",
            "Train Loss: 1.6513964809588533, Train_acc: 0.9741176470588235, Test Loss: 3.8701115449269614, Test acc: 0.5705882352941176\n",
            "Train Loss: 1.648527851745264, Train_acc: 0.9734117647058823, Test Loss: 3.814443906148275, Test acc: 0.5735294117647058\n",
            "Train Loss: 1.6459485470359005, Train_acc: 0.974, Test Loss: 3.8712004820505777, Test acc: 0.5764705882352941\n",
            "Train Loss: 1.6429171402062943, Train_acc: 0.973764705882353, Test Loss: 3.8885162671407065, Test acc: 0.5705882352941176\n",
            "Train Loss: 1.6424154221121945, Train_acc: 0.9743529411764705, Test Loss: 3.877134323120117, Test acc: 0.5735294117647058\n",
            "Train Loss: 1.6401857030925466, Train_acc: 0.974, Test Loss: 3.8794241746266684, Test acc: 0.5794117647058824\n",
            "Train Loss: 1.6365345075948914, Train_acc: 0.974, Test Loss: 3.9602421124776206, Test acc: 0.5794117647058824\n",
            "Train Loss: 1.6343753426822263, Train_acc: 0.974, Test Loss: 3.9080899556477866, Test acc: 0.5735294117647058\n",
            "Train Loss: 1.6315057135339994, Train_acc: 0.9741176470588235, Test Loss: 3.9250349203745523, Test acc: 0.5735294117647058\n",
            "Train Loss: 1.6305604461413712, Train_acc: 0.973764705882353, Test Loss: 3.9509920279184976, Test acc: 0.5794117647058824\n",
            "Train Loss: 1.6279511647437936, Train_acc: 0.9741176470588235, Test Loss: 3.972977797190348, Test acc: 0.5794117647058824\n",
            "Train Loss: 1.6255456582823795, Train_acc: 0.9741176470588235, Test Loss: 3.977734645207723, Test acc: 0.5764705882352941\n",
            "Train Loss: 1.6249021647581414, Train_acc: 0.9742352941176471, Test Loss: 3.961273272832235, Test acc: 0.5764705882352941\n",
            "Train Loss: 1.6213466676313486, Train_acc: 0.9741176470588235, Test Loss: 3.9876866340637207, Test acc: 0.5764705882352941\n",
            "Train Loss: 1.6200470728660696, Train_acc: 0.9736470588235294, Test Loss: 3.9336182276407876, Test acc: 0.5852941176470589\n",
            "Fold 3\n",
            "Train Loss: 4.398240865166508, Train_acc: 0.18505882352941178, Test Loss: 2.75400439898173, Test acc: 0.21470588235294116\n",
            "Train Loss: 4.1807208737330654, Train_acc: 0.2515294117647059, Test Loss: 2.5905823707580566, Test acc: 0.22941176470588234\n",
            "Train Loss: 3.9186414960604994, Train_acc: 0.3255294117647059, Test Loss: 2.425349712371826, Test acc: 0.27941176470588236\n",
            "Train Loss: 3.55224597987844, Train_acc: 0.4331764705882353, Test Loss: 2.243590792020162, Test acc: 0.36764705882352944\n",
            "Train Loss: 3.0344146009701403, Train_acc: 0.5962352941176471, Test Loss: 2.172152042388916, Test acc: 0.42058823529411765\n",
            "Train Loss: 2.606204879817678, Train_acc: 0.7151764705882353, Test Loss: 2.3130800326665244, Test acc: 0.4147058823529412\n",
            "Train Loss: 2.4004900348720266, Train_acc: 0.7731764705882352, Test Loss: 2.432346979777018, Test acc: 0.4764705882352941\n",
            "Train Loss: 2.3147791108088707, Train_acc: 0.8004705882352942, Test Loss: 2.3915465672810874, Test acc: 0.4676470588235294\n",
            "Train Loss: 2.176065427153858, Train_acc: 0.8378823529411765, Test Loss: 2.5672444105148315, Test acc: 0.4970588235294118\n",
            "Train Loss: 2.0489463557058305, Train_acc: 0.8698823529411764, Test Loss: 2.7400108575820923, Test acc: 0.5\n",
            "Train Loss: 1.9813999649304062, Train_acc: 0.892, Test Loss: 2.773310422897339, Test acc: 0.5147058823529411\n",
            "Train Loss: 1.9362923262724236, Train_acc: 0.9034117647058824, Test Loss: 3.165984272956848, Test acc: 0.5147058823529411\n",
            "Train Loss: 1.8846082046850403, Train_acc: 0.9216470588235294, Test Loss: 3.1161844730377197, Test acc: 0.5205882352941177\n",
            "Train Loss: 1.8640967429573856, Train_acc: 0.928, Test Loss: 3.2567232052485147, Test acc: 0.5058823529411764\n",
            "Train Loss: 1.8615932962787685, Train_acc: 0.928235294117647, Test Loss: 3.0249205827713013, Test acc: 0.5323529411764706\n",
            "Train Loss: 1.8381243268055703, Train_acc: 0.9348235294117647, Test Loss: 3.436840295791626, Test acc: 0.5294117647058824\n",
            "Train Loss: 1.7913010956636115, Train_acc: 0.9462352941176471, Test Loss: 3.550062974294027, Test acc: 0.5176470588235295\n",
            "Train Loss: 1.7776182996692942, Train_acc: 0.9484705882352941, Test Loss: 3.5088550249735513, Test acc: 0.5235294117647059\n",
            "Train Loss: 1.7660296354720841, Train_acc: 0.9514117647058824, Test Loss: 3.614660660425822, Test acc: 0.5176470588235295\n",
            "Train Loss: 1.7459065576097859, Train_acc: 0.9554117647058824, Test Loss: 3.644219080607096, Test acc: 0.5323529411764706\n",
            "Train Loss: 1.7282577415010822, Train_acc: 0.9590588235294117, Test Loss: 3.8262112935384116, Test acc: 0.5323529411764706\n",
            "Train Loss: 1.72057981455504, Train_acc: 0.9602352941176471, Test Loss: 3.830221652984619, Test acc: 0.5323529411764706\n",
            "Train Loss: 1.7143964660701467, Train_acc: 0.9609411764705882, Test Loss: 3.826992909113566, Test acc: 0.5294117647058824\n",
            "Train Loss: 1.709419163305368, Train_acc: 0.9616470588235294, Test Loss: 3.8652827739715576, Test acc: 0.5294117647058824\n",
            "Train Loss: 1.7057435779429193, Train_acc: 0.962, Test Loss: 3.90507435798645, Test acc: 0.5323529411764706\n",
            "Train Loss: 1.702803748757092, Train_acc: 0.9635294117647059, Test Loss: 3.957017183303833, Test acc: 0.5323529411764706\n",
            "Train Loss: 1.699973031656066, Train_acc: 0.963764705882353, Test Loss: 3.9304720560709634, Test acc: 0.5294117647058824\n",
            "Train Loss: 1.694571991464985, Train_acc: 0.9643529411764706, Test Loss: 3.934812863667806, Test acc: 0.5264705882352941\n",
            "Train Loss: 1.690864808523833, Train_acc: 0.9643529411764706, Test Loss: 3.889002005259196, Test acc: 0.538235294117647\n",
            "Train Loss: 1.688162294786368, Train_acc: 0.9650588235294117, Test Loss: 3.8882989088694253, Test acc: 0.538235294117647\n",
            "Train Loss: 1.6842118982058853, Train_acc: 0.9652941176470589, Test Loss: 3.95427934328715, Test acc: 0.538235294117647\n",
            "Train Loss: 1.6814203084404789, Train_acc: 0.9647058823529412, Test Loss: 3.9406418800354004, Test acc: 0.5411764705882353\n",
            "Train Loss: 1.678840050056799, Train_acc: 0.9669411764705882, Test Loss: 3.9498252073923745, Test acc: 0.538235294117647\n",
            "Train Loss: 1.676770361501779, Train_acc: 0.9667058823529412, Test Loss: 3.914560159047445, Test acc: 0.538235294117647\n",
            "Train Loss: 1.6735692255532564, Train_acc: 0.9671764705882353, Test Loss: 3.972909132639567, Test acc: 0.538235294117647\n",
            "Train Loss: 1.6693322142558311, Train_acc: 0.9675294117647059, Test Loss: 3.980564594268799, Test acc: 0.5323529411764706\n",
            "Train Loss: 1.6659481098402793, Train_acc: 0.9678823529411764, Test Loss: 4.054169813791911, Test acc: 0.538235294117647\n",
            "Train Loss: 1.663438065728145, Train_acc: 0.9678823529411764, Test Loss: 4.07726248105367, Test acc: 0.5441176470588235\n",
            "Train Loss: 1.6612800733367008, Train_acc: 0.9681176470588235, Test Loss: 4.033180634180705, Test acc: 0.538235294117647\n",
            "Train Loss: 1.65690065675707, Train_acc: 0.9690588235294118, Test Loss: 4.043496767679851, Test acc: 0.5352941176470588\n",
            "Train Loss: 1.6550247135447032, Train_acc: 0.9692941176470589, Test Loss: 4.094417492548625, Test acc: 0.5441176470588235\n",
            "Train Loss: 1.6526720203570466, Train_acc: 0.9692941176470589, Test Loss: 4.104599078496297, Test acc: 0.5441176470588235\n",
            "Train Loss: 1.6510452032089233, Train_acc: 0.9687058823529412, Test Loss: 4.113321701685588, Test acc: 0.5441176470588235\n",
            "Train Loss: 1.646654902999081, Train_acc: 0.9689411764705882, Test Loss: 4.135720570882161, Test acc: 0.5470588235294118\n",
            "Train Loss: 1.6417062602826018, Train_acc: 0.968, Test Loss: 4.076442241668701, Test acc: 0.5441176470588235\n",
            "Train Loss: 1.638788395853185, Train_acc: 0.9689411764705882, Test Loss: 4.112573782602946, Test acc: 0.538235294117647\n",
            "Train Loss: 1.6374209016116696, Train_acc: 0.9694117647058823, Test Loss: 4.127639134724935, Test acc: 0.5323529411764706\n",
            "Train Loss: 1.6366633852915977, Train_acc: 0.968, Test Loss: 4.240681489308675, Test acc: 0.5352941176470588\n",
            "Train Loss: 1.6320007914927468, Train_acc: 0.9682352941176471, Test Loss: 4.16927170753479, Test acc: 0.5352941176470588\n",
            "Train Loss: 1.628702749067278, Train_acc: 0.9698823529411764, Test Loss: 4.218063433965047, Test acc: 0.5352941176470588\n",
            "Fold 4\n",
            "Train Loss: 4.372412845269958, Train_acc: 0.18576470588235294, Test Loss: 2.770413796106974, Test acc: 0.12941176470588237\n",
            "Train Loss: 4.106393483147692, Train_acc: 0.26329411764705885, Test Loss: 2.6271007855733237, Test acc: 0.2323529411764706\n",
            "Train Loss: 3.8430770404303254, Train_acc: 0.35223529411764704, Test Loss: 2.4273252487182617, Test acc: 0.29411764705882354\n",
            "Train Loss: 3.49996350060648, Train_acc: 0.4428235294117647, Test Loss: 2.3004095554351807, Test acc: 0.37058823529411766\n",
            "Train Loss: 3.028178179441993, Train_acc: 0.5887058823529412, Test Loss: 2.181934674580892, Test acc: 0.4235294117647059\n",
            "Train Loss: 2.6411411975746724, Train_acc: 0.6998823529411765, Test Loss: 2.236664295196533, Test acc: 0.45588235294117646\n",
            "Train Loss: 2.3807472364226383, Train_acc: 0.7796470588235294, Test Loss: 2.3280309041341147, Test acc: 0.4647058823529412\n",
            "Train Loss: 2.286473704807794, Train_acc: 0.8084705882352942, Test Loss: 2.364543636639913, Test acc: 0.5\n",
            "Train Loss: 2.1667178299889636, Train_acc: 0.8402352941176471, Test Loss: 2.5027080376942954, Test acc: 0.5235294117647059\n",
            "Train Loss: 2.096291340998749, Train_acc: 0.859764705882353, Test Loss: 2.574764847755432, Test acc: 0.5294117647058824\n",
            "Train Loss: 1.993418590346379, Train_acc: 0.8925882352941177, Test Loss: 2.5588264067967734, Test acc: 0.5764705882352941\n",
            "Train Loss: 1.9515266720928364, Train_acc: 0.9071764705882353, Test Loss: 2.8839337825775146, Test acc: 0.5588235294117647\n",
            "Train Loss: 1.9053623889809224, Train_acc: 0.916235294117647, Test Loss: 2.8706087668736777, Test acc: 0.5705882352941176\n",
            "Train Loss: 1.8826009337581806, Train_acc: 0.9229411764705883, Test Loss: 2.8721725145975747, Test acc: 0.5852941176470589\n",
            "Train Loss: 1.843253365203516, Train_acc: 0.9344705882352942, Test Loss: 3.013416886329651, Test acc: 0.5764705882352941\n",
            "Train Loss: 1.8175816108931357, Train_acc: 0.9402352941176471, Test Loss: 3.081753452618917, Test acc: 0.5911764705882353\n",
            "Train Loss: 1.7724086989217729, Train_acc: 0.9535294117647058, Test Loss: 3.173470417658488, Test acc: 0.5941176470588235\n",
            "Train Loss: 1.7529427026634785, Train_acc: 0.9588235294117647, Test Loss: 3.3091882864634194, Test acc: 0.5911764705882353\n",
            "Train Loss: 1.734807042933222, Train_acc: 0.9629411764705882, Test Loss: 3.275980234146118, Test acc: 0.5911764705882353\n",
            "Train Loss: 1.7246772335536444, Train_acc: 0.9635294117647059, Test Loss: 3.385418693224589, Test acc: 0.6029411764705882\n",
            "Train Loss: 1.7198975139589452, Train_acc: 0.9631764705882353, Test Loss: 3.374850352605184, Test acc: 0.6\n",
            "Train Loss: 1.7146875609212846, Train_acc: 0.9643529411764706, Test Loss: 3.3906362056732178, Test acc: 0.5911764705882353\n",
            "Train Loss: 1.7293682400860004, Train_acc: 0.959764705882353, Test Loss: 3.474045912424723, Test acc: 0.5970588235294118\n",
            "Train Loss: 1.7431823050797874, Train_acc: 0.9564705882352941, Test Loss: 3.4250017007191977, Test acc: 0.5882352941176471\n",
            "Train Loss: 1.73482513783583, Train_acc: 0.9570588235294117, Test Loss: 3.371687571207682, Test acc: 0.5911764705882353\n",
            "Train Loss: 1.735166154690643, Train_acc: 0.9584705882352941, Test Loss: 3.3624324798583984, Test acc: 0.6029411764705882\n",
            "Train Loss: 1.720592431168058, Train_acc: 0.9634117647058823, Test Loss: 3.333441893259684, Test acc: 0.5823529411764706\n",
            "Train Loss: 1.7025187264627486, Train_acc: 0.9669411764705882, Test Loss: 3.449110984802246, Test acc: 0.5882352941176471\n",
            "Train Loss: 1.6932319473864428, Train_acc: 0.9678823529411764, Test Loss: 3.519641081492106, Test acc: 0.5852941176470589\n",
            "Train Loss: 1.6879011499347971, Train_acc: 0.9671764705882353, Test Loss: 3.4905415376027427, Test acc: 0.5852941176470589\n",
            "Train Loss: 1.679922342300415, Train_acc: 0.9682352941176471, Test Loss: 3.5271392663319907, Test acc: 0.5941176470588235\n",
            "Train Loss: 1.675872745798595, Train_acc: 0.9687058823529412, Test Loss: 3.5828733444213867, Test acc: 0.5941176470588235\n",
            "Train Loss: 1.6728731322644361, Train_acc: 0.9683529411764706, Test Loss: 3.5879921118418374, Test acc: 0.5941176470588235\n",
            "Train Loss: 1.6702565061512278, Train_acc: 0.9683529411764706, Test Loss: 3.6777255535125732, Test acc: 0.5911764705882353\n",
            "Train Loss: 1.6675358868356962, Train_acc: 0.9684705882352941, Test Loss: 3.6625418663024902, Test acc: 0.5911764705882353\n",
            "Train Loss: 1.6628643665740739, Train_acc: 0.9692941176470589, Test Loss: 3.763110796610514, Test acc: 0.5823529411764706\n",
            "Train Loss: 1.6597068825764443, Train_acc: 0.9688235294117648, Test Loss: 3.7358214060465493, Test acc: 0.5882352941176471\n",
            "Train Loss: 1.6575597933868864, Train_acc: 0.9689411764705882, Test Loss: 3.7331291834513345, Test acc: 0.5911764705882353\n",
            "Train Loss: 1.6560975135262332, Train_acc: 0.9695294117647059, Test Loss: 3.7155357201894126, Test acc: 0.5941176470588235\n",
            "Train Loss: 1.6512805952954648, Train_acc: 0.969764705882353, Test Loss: 3.825749079386393, Test acc: 0.5941176470588235\n",
            "Train Loss: 1.6504787509121113, Train_acc: 0.9692941176470589, Test Loss: 3.770381132761637, Test acc: 0.5970588235294118\n",
            "Train Loss: 1.6464661840182633, Train_acc: 0.9701176470588235, Test Loss: 3.774974266688029, Test acc: 0.5970588235294118\n",
            "Train Loss: 1.6441756682609445, Train_acc: 0.9701176470588235, Test Loss: 3.8039942582448325, Test acc: 0.5970588235294118\n",
            "Train Loss: 1.6417197807511288, Train_acc: 0.969764705882353, Test Loss: 3.7723122437795005, Test acc: 0.6029411764705882\n",
            "Train Loss: 1.6399906339930064, Train_acc: 0.9703529411764706, Test Loss: 3.7512638568878174, Test acc: 0.6029411764705882\n",
            "Train Loss: 1.6378460126136667, Train_acc: 0.9707058823529412, Test Loss: 3.857515573501587, Test acc: 0.5970588235294118\n",
            "Train Loss: 1.635687522034147, Train_acc: 0.9704705882352941, Test Loss: 3.8652267456054688, Test acc: 0.5970588235294118\n",
            "Train Loss: 1.6330670652104848, Train_acc: 0.9702352941176471, Test Loss: 3.854696194330851, Test acc: 0.5970588235294118\n",
            "Train Loss: 1.63043212890625, Train_acc: 0.9707058823529412, Test Loss: 3.879237333933512, Test acc: 0.5970588235294118\n",
            "Train Loss: 1.6284844341562754, Train_acc: 0.9704705882352941, Test Loss: 3.8336167335510254, Test acc: 0.6029411764705882\n",
            "Fold 5\n",
            "Train Loss: 4.3833940135898875, Train_acc: 0.18341176470588236, Test Loss: 2.7731099128723145, Test acc: 0.13823529411764707\n",
            "Train Loss: 4.1264678150860234, Train_acc: 0.2707058823529412, Test Loss: 2.69072159131368, Test acc: 0.15294117647058825\n",
            "Train Loss: 3.828697040899476, Train_acc: 0.33752941176470586, Test Loss: 2.6238465309143066, Test acc: 0.2088235294117647\n",
            "Train Loss: 3.4622482719706067, Train_acc: 0.4481176470588235, Test Loss: 2.50764536857605, Test acc: 0.32941176470588235\n",
            "Train Loss: 2.998151718680538, Train_acc: 0.6032941176470589, Test Loss: 2.4615794022878013, Test acc: 0.4117647058823529\n",
            "Train Loss: 2.5937006829389886, Train_acc: 0.7272941176470589, Test Loss: 2.5146615505218506, Test acc: 0.45\n",
            "Train Loss: 2.4074643939288696, Train_acc: 0.7763529411764706, Test Loss: 2.5505968729654946, Test acc: 0.4411764705882353\n",
            "Train Loss: 2.214437292582953, Train_acc: 0.8267058823529412, Test Loss: 2.7745958964029946, Test acc: 0.4676470588235294\n",
            "Train Loss: 2.049755393569149, Train_acc: 0.8738823529411764, Test Loss: 3.0031845569610596, Test acc: 0.4823529411764706\n",
            "Train Loss: 2.044055682509693, Train_acc: 0.8736470588235294, Test Loss: 3.104311386744181, Test acc: 0.4764705882352941\n",
            "Train Loss: 2.0043969670338417, Train_acc: 0.8874117647058823, Test Loss: 3.061219056447347, Test acc: 0.5294117647058824\n",
            "Train Loss: 1.9464954440273456, Train_acc: 0.9035294117647059, Test Loss: 3.0044793287913003, Test acc: 0.5147058823529411\n",
            "Train Loss: 1.9078406113297193, Train_acc: 0.912235294117647, Test Loss: 3.063969055811564, Test acc: 0.5235294117647059\n",
            "Train Loss: 1.8352435115558, Train_acc: 0.9331764705882353, Test Loss: 3.4333898226420083, Test acc: 0.538235294117647\n",
            "Train Loss: 1.82319291669931, Train_acc: 0.9378823529411765, Test Loss: 3.2181762059529624, Test acc: 0.5147058823529411\n",
            "Train Loss: 1.8151550524270357, Train_acc: 0.9390588235294117, Test Loss: 3.363477865854899, Test acc: 0.5411764705882353\n",
            "Train Loss: 1.7785912556434744, Train_acc: 0.9494117647058824, Test Loss: 3.5811002254486084, Test acc: 0.5294117647058824\n",
            "Train Loss: 1.7481759128285879, Train_acc: 0.9555294117647058, Test Loss: 3.579529285430908, Test acc: 0.5441176470588235\n",
            "Train Loss: 1.7336791824938647, Train_acc: 0.9571764705882353, Test Loss: 3.718390385309855, Test acc: 0.5441176470588235\n",
            "Train Loss: 1.7287396263720385, Train_acc: 0.9575294117647059, Test Loss: 3.7108476956685386, Test acc: 0.5470588235294118\n",
            "Train Loss: 1.7239258396091746, Train_acc: 0.9576470588235294, Test Loss: 3.7157347997029624, Test acc: 0.5470588235294118\n",
            "Train Loss: 1.7190396714566358, Train_acc: 0.9576470588235294, Test Loss: 3.7575923601786294, Test acc: 0.5411764705882353\n",
            "Train Loss: 1.7148967589905013, Train_acc: 0.9582352941176471, Test Loss: 3.763512134552002, Test acc: 0.55\n",
            "Train Loss: 1.7106271323873037, Train_acc: 0.9589411764705882, Test Loss: 3.848810911178589, Test acc: 0.5470588235294118\n",
            "Train Loss: 1.7058243377884823, Train_acc: 0.9596470588235294, Test Loss: 3.8051973978678384, Test acc: 0.5470588235294118\n",
            "Train Loss: 1.702544135833854, Train_acc: 0.9595294117647059, Test Loss: 3.8919596672058105, Test acc: 0.5441176470588235\n",
            "Train Loss: 1.6957395699486804, Train_acc: 0.961764705882353, Test Loss: 3.9486798445383706, Test acc: 0.5352941176470588\n",
            "Train Loss: 1.6913248780948014, Train_acc: 0.9641176470588235, Test Loss: 3.990361054738363, Test acc: 0.5352941176470588\n",
            "Train Loss: 1.6890600030101948, Train_acc: 0.9634117647058823, Test Loss: 4.004949728647868, Test acc: 0.5352941176470588\n",
            "Train Loss: 1.6835544127136914, Train_acc: 0.964, Test Loss: 3.9185925324757895, Test acc: 0.5441176470588235\n",
            "Train Loss: 1.6793101154156584, Train_acc: 0.9641176470588235, Test Loss: 4.0697021484375, Test acc: 0.55\n",
            "Train Loss: 1.6774760751581903, Train_acc: 0.9635294117647059, Test Loss: 4.031771739323934, Test acc: 0.538235294117647\n",
            "Train Loss: 1.6722727907237722, Train_acc: 0.963764705882353, Test Loss: 4.119023323059082, Test acc: 0.5441176470588235\n",
            "Train Loss: 1.6681361803367956, Train_acc: 0.9643529411764706, Test Loss: 4.0411685307820635, Test acc: 0.5294117647058824\n",
            "Train Loss: 1.664949027460013, Train_acc: 0.9664705882352941, Test Loss: 4.18175212542216, Test acc: 0.5411764705882353\n",
            "Train Loss: 1.6608524856282705, Train_acc: 0.9663529411764706, Test Loss: 4.216400782267253, Test acc: 0.5235294117647059\n",
            "Train Loss: 1.6583514142392286, Train_acc: 0.9668235294117647, Test Loss: 4.177926381429036, Test acc: 0.5264705882352941\n",
            "Train Loss: 1.654544876582587, Train_acc: 0.9687058823529412, Test Loss: 4.155136823654175, Test acc: 0.5264705882352941\n",
            "Train Loss: 1.6529935153562632, Train_acc: 0.9669411764705882, Test Loss: 4.114807764689128, Test acc: 0.5323529411764706\n",
            "Train Loss: 1.6497488626793249, Train_acc: 0.9685882352941176, Test Loss: 4.134519497553508, Test acc: 0.5323529411764706\n",
            "Train Loss: 1.6473256990091125, Train_acc: 0.9687058823529412, Test Loss: 4.249003966649373, Test acc: 0.5411764705882353\n",
            "Train Loss: 1.644644303108329, Train_acc: 0.9684705882352941, Test Loss: 4.173516035079956, Test acc: 0.5411764705882353\n",
            "Train Loss: 1.6413509151828822, Train_acc: 0.9689411764705882, Test Loss: 4.344666004180908, Test acc: 0.538235294117647\n",
            "Train Loss: 1.6402734909484635, Train_acc: 0.9685882352941176, Test Loss: 4.274111906687419, Test acc: 0.5235294117647059\n",
            "Train Loss: 1.63700149486314, Train_acc: 0.9690588235294118, Test Loss: 4.279438813527425, Test acc: 0.5352941176470588\n",
            "Train Loss: 1.6447356690221757, Train_acc: 0.9665882352941176, Test Loss: 4.103467067082723, Test acc: 0.5352941176470588\n",
            "Train Loss: 1.6392933076886989, Train_acc: 0.9676470588235294, Test Loss: 4.2370985349019366, Test acc: 0.5294117647058824\n",
            "Train Loss: 1.6325246487090836, Train_acc: 0.9684705882352941, Test Loss: 4.17589004834493, Test acc: 0.5352941176470588\n",
            "Train Loss: 1.6293540161047408, Train_acc: 0.9688235294117648, Test Loss: 4.13627290725708, Test acc: 0.5264705882352941\n",
            "Train Loss: 1.6271717566162793, Train_acc: 0.9690588235294118, Test Loss: 4.219584941864014, Test acc: 0.5264705882352941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save models\n",
        "\n",
        "# import pickle\n",
        "# file=open('/content/drive/MyDrive/models','wb')\n",
        "# pickle.dump(models, file)\n",
        "# file.close()"
      ],
      "metadata": {
        "id": "b8fGtv3Wgn-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the average accuray of the 5 models\n",
        "\n",
        "# import pickle\n",
        "# import sklearn\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n",
        "# file=open('/content/drive/MyDrive/models','rb')\n",
        "# models = pickle.load(file)\n",
        "# file.close()"
      ],
      "metadata": {
        "id": "FGq4yRd9qd-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the average accuray of the 5 models\n",
        "X_train_v, X_test, y_train_v, y_test = train_test_split(X, y, test_size=0.2, random_state=99, stratify=y)\n",
        "t = pd.DataFrame(y_test, columns = ['class']).reset_index()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "idx = []\n",
        "y_test = []\n",
        "for i in range(0,18):\n",
        "    if i!=label_dict['Unknown']:\n",
        "      idx += list(t[t['class']==i].sample(20,replace=True)['index'])\n",
        "      y_test += [i]*20\n",
        "X_test =  X_test[idx]\n",
        "X_test = torch.as_tensor(X_test, dtype=torch.float)\n",
        "y_test = torch.as_tensor(y_test, dtype=torch.float).type(torch.LongTensor)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "acc = []\n",
        "for best_model in models:\n",
        "  acc.append(valid_epoch(best_model[0],device,test_dataloader,criterion)[1])"
      ],
      "metadata": {
        "id": "Prc3ht2anMZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N_ChcrG70nP",
        "outputId": "40379a15-f402-4fcf-c863-aa35074db819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6176470588235294"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy with majority vote\n",
        "\n",
        "outputs = []\n",
        "with torch.no_grad():\n",
        "  for model in models:\n",
        "    model[0].eval()\n",
        "    model_output = []\n",
        "    label_= []\n",
        "    model[0].to(device)\n",
        "    for inputs, labels in test_dataloader:\n",
        "        inputs = inputs.reshape(-1,512,128).to(device)\n",
        "        labels = labels.to(device)\n",
        "        cnnOutputs = model[0](inputs)\n",
        "        model_output.append(cnnOutputs)\n",
        "        label_.append(labels)\n",
        "        # del inputs\n",
        "    outputs.append(torch.vstack(model_output))"
      ],
      "metadata": {
        "id": "EhEUUD5ZgLgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shape = outputs[0].shape\n",
        "labels = torch.hstack(label_)\n",
        "cnnresult = torch.vstack(outputs).reshape(-1,shape[0],shape[1]).mean(axis=0).to('cpu')\n",
        "_, cnnPredicted = torch.max(cnnresult.data, 1)\n",
        "cnnCorrect = (cnnPredicted.detach().numpy() == labels.to('cpu').detach().numpy()).sum().item()\n",
        "print(cnnCorrect/len(labels)*100)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confu_m = confusion_matrix(labels.to('cpu'), cnnPredicted.to('cpu'))\n",
        "import seaborn as sns\n",
        "sns.heatmap(confu_m,square=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "lQew8gS7o7SU",
        "outputId": "cfdd3ba9-a3be-446f-a262-716f25e59d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66.47058823529412\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgwAAAGiCAYAAACLeJ4MAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABGtUlEQVR4nO3de1yUZf4//tdwGljkoCIwaKKICnlARSU8RCYfEF0TKw+spWbZNxdblZWMNUWzGg9blqvp1qZYrofaVTMrDAlPK55AtuxAoAgqB0XjNMaIzP37Y39OzQHGm7lvZ8DXcx/X49Hc132/5y06O2+u67qvWyEIggAiIiKiZjjYOgEiIiKyfywYiIiIyCIWDERERGQRCwYiIiKyiAUDERERWcSCgYiIiCxiwUBEREQWsWAgIiIii1gwEBERkUUsGIiIiMgiFgxERER2Qq1WY8iQIfDw8ICvry/i4+ORn59vcE59fT0SExPRsWNHtGvXDk888QQqKiqajSsIApYuXQqVSgU3NzdER0ejoKBAVG4sGIiIiOzE4cOHkZiYiBMnTiAjIwMNDQ2IiYmBRqPRn7NgwQJ89tln+OSTT3D48GGUlpbi8ccfbzbu6tWrsW7dOmzatAknT56Eu7s7YmNjUV9ff9e5KfjwKSIiIvt07do1+Pr64vDhw3j44YdRXV2NTp06Yfv27XjyyScBAD/++CNCQ0ORnZ2Nhx56yCSGIAgICAjAn//8ZyxcuBAAUF1dDT8/P6SlpWHq1Kl3lQtHGIiIiGSk1WpRU1Nj0LRa7V1dW11dDQDo0KEDACAnJwcNDQ2Ijo7WnxMSEoKuXbsiOzvbbIyioiKUl5cbXOPl5YWIiIgmrzHH6a7PlNkvHyyULbbHnB2yxZZbkJdKttgXqstki010v+Bn1DZu37oia/yGyguSxVKv/xDLly83OJaamoply5Y1e51Op8P8+fMxfPhw9O3bFwBQXl4OFxcXeHt7G5zr5+eH8vJys3HuHPfz87vra8yxm4KBiIjIbugaJQuVkpKCpKQkg2NKpdLidYmJiTh37hyOHTsmWS7W4JQEERGRjJRKJTw9PQ2apYJh7ty52L9/P7KystClSxf9cX9/f9y6dQtVVVUG51dUVMDf399srDvHje+kaO4ac1gwEBERGRN00jUxbysImDt3Lvbs2YOvv/4a3bt3N+gPDw+Hs7MzMjMz9cfy8/NRUlKCyMhIszG7d+8Of39/g2tqampw8uTJJq8xh1MSRERExnTivuilkpiYiO3bt+PTTz+Fh4eHfo2Bl5cX3Nzc4OXlhWeffRZJSUno0KEDPD098eKLLyIyMtLgDomQkBCo1WpMnDgRCoUC8+fPx2uvvYaePXuie/fuWLJkCQICAhAfH3/XubFgICIiMiKIHBmQysaNGwEAjzzyiMHxLVu2YObMmQCAtWvXwsHBAU888QS0Wi1iY2Px7rvvGpyfn5+vv8MCAF566SVoNBo8//zzqKqqwogRI5Ceng5XV9e7zs1u9mHgXRLmcQU2kX3jZ9Q25L5L4lbpd5LFcgnoI1ksW+IIAxERkTEbTUnYMxYMRERExmw0JWHPRBcMlZWV2Lx5M7Kzs/WLMfz9/TFs2DDMnDkTnTp1kjxJIiIisi1RBcPp06cRGxuL3/3ud4iOjkavXr0A/O9eznXr1mHlypU4cOAABg8e3GwcrVZrsi2mruE2lM4c8CAiIjsg4cZNbYWob+gXX3wRkyZNwqZNm6BQKAz6BEHACy+8gBdffNHi3tRqtdpkm8y/jI/EKxOGiUmHiIhIHpySMCHqLgk3NzecPXsWISEhZvt//PFHDBw4EL/88kuzccyOMGxfKtsIA++SMI8rsImsx8+obch+l8TFM5LFcunW/Kh7ayHqG9rf3x+nTp1qsmA4deqUycMtzFEqlSbbYv7C6QgiIrIXvEvChKhv6YULF+L5559HTk4ORo8erS8OKioqkJmZiffffx9//etfZUmUiIjoXrHVxk32TFTBkJiYCB8fH6xduxbvvvsuGhv/tyjE0dER4eHhSEtLw+TJk2VJlIiIiGxH9DzAlClTMGXKFDQ0NKCyshIA4OPjA2dnZ8mTIyIisglOSZho8cIBZ2dnqFTyLfYhIiKyGU5JmOBKQyIiImPch8GEg60TICIiIvvHEQYiIiJjnJIwYTcFg5ybK9WsiJEttueSr2SLDQA3tDWyxici63R08ZAt9gW03o2bvF3dbZ2Cdbjo0QSnJIiIiMgiuxlhICIishuckjDBgoGIiMgYpyRMcEqCiIiILOIIAxERkRFB4D4MxlgwEBERGeMaBhOckiAiIiKLOMJARERkjIseTbBgICIiMsYpCRM2KRi0Wi20Wq3BMUEQoFAobJEOERGRIT58yoTkaxguXbqEWbNmNXuOWq2Gl5eXQRN0tVKnQkRERBKRvGC4ceMGtm7d2uw5KSkpqK6uNmgKB/n2YyciIhJF0EnX2gjRUxL79u1rtv/ChQsWYyiVSiiVSoNjnI4gIiK7wUWPJkQXDPHx8VAoFBAEoclz+OVPRETUtoieklCpVNi9ezd0Op3ZlpubK0eeRERE9w6nJEyILhjCw8ORk5PTZL+l0QciIiK7p9NJ19oI0VMSycnJ0Gg0TfYHBwcjKyvLqqSIiIjIvoguGEaOHNlsv7u7O6KiolqcEBERkc21oZEBqXCnRyIiIiN8WqUpPnyKiIiILGLBQEREZMxGix6PHDmC8ePHIyAgAAqFAnv37jXoVygUZtuaNWuajLls2TKT80NCQkT/SO6LKYkpG67JFrvyiV6yxQYAn3//JGt8IiI5VNU3vTi+VbDR7ZAajQZhYWGYNWsWHn/8cZP+srIyg9dffvklnn32WTzxxBPNxu3Tpw8OHjyof+3kJP7r/74oGIiIiESx0aLHuLg4xMXFNdnv7+9v8PrTTz/FqFGjEBQU1GxcJycnk2vF4pQEERGRjLRaLWpqagya8RObW6KiogKff/45nn32WYvnFhQUICAgAEFBQZg2bRpKSkpEvx8LBiIiImMS7vRo7gnNarXa6hS3bt0KDw8Ps1MXvxUREYG0tDSkp6dj48aNKCoqwsiRI1FbK+4p0ZySICIiMibhlERKSgqSkpIMjhk/gLElNm/ejGnTpsHV1bXZ8347xdG/f39EREQgMDAQH3/88V2NTtzBgoGIiEhG5p7QbK2jR48iPz8fu3btEn2tt7c3evXqhcLCQlHXcUqCiIjImJ0/fOqDDz5AeHg4wsLCRF9bV1eH8+fPQ6VSibqOBQMREZExG+3DUFdXh7y8POTl5QEAioqKkJeXZ7BIsaamBp988gmee+45szFGjx6N9evX618vXLgQhw8fxsWLF3H8+HFMnDgRjo6OSEhIEJUbpySIiIjsxJkzZzBq1Cj96ztrH2bMmIG0tDQAwM6dOyEIQpNf+OfPn0dlZaX+9eXLl5GQkIDr16+jU6dOGDFiBE6cOIFOnTqJyk0h2MmzqJ1cOssWO85/oGyxPxou7+Yk3LiJyL4N6STf5m2nr/Hz35Tbt67IGv+Xz9+WLJbbuPmSxbIl0VMSv/zyC44dO4bvv//epK++vh4ffvihxRjm7km1k7qFiIjI7tcw2IKoguGnn35CaGgoHn74YfTr1w9RUVEG21RWV1fjmWeesRjH3D2pgk7c/aBERER074gqGBYtWoS+ffvi6tWryM/Ph4eHB4YPHy56x6iUlBRUV1cbNIWDh6gYREREsrHRokd7JmrR4/Hjx3Hw4EH4+PjAx8cHn332Gf74xz9i5MiRyMrKgru7+13FMXdPqkKhEJMKERGRfNrQVIJURI0w/PLLLwZPuFIoFNi4cSPGjx+PqKgo/PQTF+gQEVEbwBEGE6JGGEJCQnDmzBmEhoYaHL9zv+djjz0mXWZERERkN0SNMEycOBE7duww27d+/XokJCTwbgciImr9eJeECVEFQ0pKCr744osm+999913o2tDwCxER3ac4JWGCW0MTERGRRdwamoiIyFgbGhmQCgsGIiIiY1yPZ4JTEkRERGTRfTHC8GX5Wdli+/xbttAAgF9Kj8oW2y1gpGyxie4XfEBUG8UpCRP3RcFAREQkCgsGE5ySICIiIos4wkBERGSsDW24JBUWDERERMY4JWGCBQMREZEx3lZpgmsYiIiIyCKOMBARERnjlIQJFgxERETGWDCY4JQEERERWSR6hOGHH37AiRMnEBkZiZCQEPz444945513oNVq8dRTT+HRRx+1GEOr1UKr1RocEwQBCoVCbDpERETS422VJkSNMKSnp2PAgAFYuHAhBg4ciPT0dDz88MMoLCxEcXExYmJi8PXXX1uMo1ar4eXlZdAEXW2L/xBERERSEnSCZK2tEFUwvPrqq0hOTsb169exZcsW/OEPf8Ds2bORkZGBzMxMJCcnY+XKlRbjpKSkoLq62qApHDxa/IcgIiIieYkqGL777jvMnDkTADB58mTU1tbiySef1PdPmzYN33zzjcU4SqUSnp6eBo3TEUREZDd0OulaGyF6DcOdL3YHBwe4urrCy8tL3+fh4YHq6mrpsiMiIrIFrmEwIWqEoVu3bigoKNC/zs7ORteuXfWvS0pKoFKppMuOiIiI7IKoEYY5c+agsbFR/7pv374G/V9++eVd3SVBRERk19rQYkWpiCoYXnjhhWb733jjDauSISIisgttaO2BVLjTIxERkTEWDCa40yMRERFZxBEGIiIiY3y8tQkWDERERMY4JWHivigYvF3dZYtdVa+RLTYAuAWMlC32Cd8hssV+6Opp2WIT2ZOkgIdli/1W6RHZYsttSKdetk6BJMY1DERERMZ0gnRNhCNHjmD8+PEICAiAQqHA3r17DfpnzpwJhUJh0MaMGWMx7oYNG9CtWze4uroiIiICp06dEpUXwIKBiIjIlKCTromg0WgQFhaGDRs2NHnOmDFjUFZWpm87duxoNuauXbuQlJSE1NRU5ObmIiwsDLGxsbh69aqo3O6LKQkiIqLWIC4uDnFxcc2eo1Qq4e/vf9cx33rrLcyePRvPPPMMAGDTpk34/PPPsXnzZrz88st3HYcjDERERMYknJLQarWoqakxaFqttsWpHTp0CL6+vujduzfmzJmD69evN3nurVu3kJOTg+joaP0xBwcHREdHIzs7W9T7smAgIiIyIuh0kjW1Wg0vLy+DplarW5TXmDFj8OGHHyIzMxOrVq3C4cOHERcXZ/DYht+qrKxEY2Mj/Pz8DI77+fmhvLxc1HtzSoKIiEhGKSkpSEpKMjimVCpbFGvq1Kn6/+7Xrx/69++PHj164NChQxg9erRVeVrCgoGIiMiYhA+fUiqVLS4QLAkKCoKPjw8KCwvNFgw+Pj5wdHRERUWFwfGKigpR6yAAiaYkBO6IRUREbYmN7pIQ6/Lly7h+/TpUKpXZfhcXF4SHhyMzM1N/TKfTITMzE5GRkaLeS5KCQalU4ocffpAiFBERke3ZaB+Guro65OXlIS8vDwBQVFSEvLw8lJSUoK6uDsnJyThx4gQuXryIzMxMTJgwAcHBwYiNjdXHGD16NNavX69/nZSUhPfffx9bt27FDz/8gDlz5kCj0ejvmrhboqYkjOdg7mhsbMTKlSvRsWNHAP+7haM5Wq3WZIWoIAhQKBRi0iEiImpTzpw5g1GjRulf3/nenTFjBjZu3IhvvvkGW7duRVVVFQICAhATE4MVK1YYTHmcP38elZWV+tdTpkzBtWvXsHTpUpSXl2PAgAFIT083WQhpiaiC4e2330ZYWBi8vb0NjguCgB9++AHu7u539aWvVquxfPlyg2MKh3ZQOHqKSYeIiEgeNnqWxCOPPNLsNP+BAwcsxrh48aLJsblz52Lu3LnWpCauYHjjjTfw3nvv4c0338Sjjz6qP+7s7Iy0tDQ8+OCDdxXH3IrR9h1DxKRCREQkHwkXPbYVotYwvPzyy9i1axfmzJmDhQsXoqGhoUVvqlQq4enpadA4HUFERGS/RC96HDJkCHJycnDt2jUMHjwY586d45c9ERG1La3kLol7qUX7MLRr1w5bt27Fzp07ER0d3eQOU0RERK0SpyRMWLVx09SpUzFixAjk5OQgMDBQqpyIiIjIzli902OXLl3QpUsXKXIhIiKyC4KN7pKwZ9wamoiIyBinJEzwaZVERERkEUcYiIiIjHGEwcR9UTBEeveSLfaX5Wdliy23h66eli12kJf5B6FI5UJ1mWyxvV3dZYtdVa+RLTYADOkk37/109d+ki223OT8O+3b4Chb7NasoPaKrVOwThu6HVIq90XBQEREJApHGExwDQMRERFZxBEGIiIiIwJHGEywYCAiIjLGgsEEpySIiIjIIo4wEBERGeNOjyZYMBARERnjlIQJTkkQERGRRRxhICIiMsYRBhNWFQwajQYff/wxCgsLoVKpkJCQgI4dO1q8TqvVQqvVGhwTBAEKhcKadIiIiCQhCCwYjImaknjwwQdx48YNAMClS5fQt29fLFiwABkZGUhNTcWDDz6IoqIii3HUajW8vLwMmqCrbdmfgIiIiGQnqmD48ccfcfv2bQBASkoKAgICUFxcjFOnTqG4uBj9+/fH4sWLLcZJSUlBdXW1QVM4eLTsT0BERCQ1nSBdayNaPCWRnZ2NTZs2wcvLCwDQrl07LF++HFOnTrV4rVKphFKpNDjG6QgiIrIbbeiLXiqiC4Y7X+z19fVQqQyfSNi5c2dcu3ZNmsyIiIhshFtDmxJdMIwePRpOTk6oqalBfn4++vbtq+8rLi6+q0WPRERE1LqIKhhSU1MNXrdr187g9WeffYaRI0danxUREZEtcYTBhFUFg7E1a9ZYlQwREZFd4M7QJrjTIxEREVnEnR6JiIiMcNGjKRYMRERExlgwmOCUBBEREVmkEOxkw2wnl862ToFakR+D+1o+qYUeumx5e/OWqqrXyBabSKzNnUbJFnvWtSzZYgPA7VtXZI1fNUW6n433Lnl/FvcKpySIiIiMcA2DKU5JEBERkUUcYSAiIjLGfRhMsGAgIiIywikJU5ySICIiMqaTsIlw5MgRjB8/HgEBAVAoFNi7d6++r6GhAYsWLUK/fv3g7u6OgIAATJ8+HaWlpc3GXLZsGRQKhUELCQkRlxhYMBAREdkNjUaDsLAwbNiwwaTv5s2byM3NxZIlS5Cbm4vdu3cjPz8fjz32mMW4ffr0QVlZmb4dO3ZMdG6ckiAiIjIiSLiGQavVQqvVGhxTKpVQKpUm58bFxSEuLs5sHC8vL2RkZBgcW79+PYYOHYqSkhJ07dq1yRycnJzg7+/fgux/xREGIiIiYxJOSajVanh5eRk0tVotSZrV1dVQKBTw9vZu9ryCggIEBAQgKCgI06ZNQ0lJiej34ggDERGRjFJSUpCUlGRwzNzoglj19fVYtGgREhIS4Onp2eR5ERERSEtLQ+/evVFWVobly5dj5MiROHfuHDw8PO76/UQVDLm5uWjfvj26d+8OAPjoo4+wadMmlJSUIDAwEHPnzsXUqVMtxjE3PCMIAhQKhZh0iIiIZCHllERT0w/WaGhowOTJkyEIAjZu3Njsub+d4ujfvz8iIiIQGBiIjz/+GM8+++xdv6eoKYlnnnkG58+fBwD84x//wP/7f/8PgwcPxuLFizFkyBDMnj0bmzdvthjH3PCMoKsVkwoREZF8bHSXxN24UywUFxcjIyOj2dEFc7y9vdGrVy8UFhaKuk7UCENBQQF69uwJAHj33XfxzjvvYPbs2fr+IUOG4PXXX8esWbOajWNueKZ9R/G3eBAREd1P7hQLBQUFyMrKQseOHUXHqKurw/nz5/H000+Luk5UwfC73/0OlZWVCAwMxJUrVzB06FCD/oiICBQVWX5wj7nhGU5HEBGRvZBySkKMuro6g9/8i4qKkJeXhw4dOkClUuHJJ59Ebm4u9u/fj8bGRpSXlwMAOnToABcXFwDA6NGjMXHiRMydOxcAsHDhQowfPx6BgYEoLS1FamoqHB0dkZCQICo3UVMScXFx+rmSqKgo/Otf/zLo//jjjxEcHCwqASIiInsj6KRrYpw5cwYDBw7EwIEDAQBJSUkYOHAgli5diitXrmDfvn24fPkyBgwYAJVKpW/Hjx/Xxzh//jwqKyv1ry9fvoyEhAT07t0bkydPRseOHXHixAl06tRJVG6iRhhWrVqF4cOHIyoqCoMHD8abb76JQ4cOITQ0FPn5+Thx4gT27NkjKgEiIiJ7Y6sRhkceeQSC0PS21M313XHx4kWD1zt37rQ2LQAiRxgCAgJw9uxZREZGIj09HYIg4NSpU/jqq6/QpUsX/Oc//8HYsWMlSYyIiIjsh+h9GLy9vbFy5UqsXLlSjnyIiIhsT+C6OmPcuImIiMiIraYk7Bm3hiYiIiKLOMJARERkRNBxSsIYCwYiIiIjnJIwxYLBznm7ussWu6peI1tsuYUUnpMt9o/BfWWLLWfecuO/RfOSAh6WLfZbpUdkiw0ASbWnZI1PbQsLBiIiIiMC75IwwYKBiIjICKckTPEuCSIiIrKIIwxERERGeJeEKRYMRERERu7ikQ33HRYMRERERjjCYIprGIiIiMgijjAQEREZ4QiDKRYMRERERriGwZSoKYkXX3wRR48etfpNtVotampqDJrAvx0iIiK7Japg2LBhAx555BH06tULq1atQnl5eYveVK1Ww8vLy6AJutoWxSIiIpKaoFNI1toK0Ysev/rqK4wdOxZ//etf0bVrV0yYMAH79++HTnf322KlpKSgurraoCkcPMSmQkREJAtBUEjW2grRBUO/fv3w9ttvo7S0FNu2bYNWq0V8fDweeOABLF68GIWFhRZjKJVKeHp6GjSFou38UImIiNqaFt9W6ezsjMmTJyM9PR0XLlzA7Nmz8c9//hO9e/eWMj8iIqJ7TtBJ19oKSfZh6Nq1K5YtW4aioiKkp6dLEZKIiMhmdIJCstZWiCoYAgMD4ejo2GS/QqHA//3f/1mdFBEREdkXUfswFBUVyZUHERGR3WhLixWlwo2biIiIjLSl2yGlwoKBiIjICPcSNMWHTxEREZFFHGEgIiIywikJUywY7FxVvcbWKdx3Hros3+LeH4P7yhYbkDd3/luk+0lbuh1SKpySICIiIos4wkBERGSEt1WaYsFARERkhHdJmOKUBBEREVnEEQYiIiIjXPRoigUDERGREa5hMMUpCSIiIjtx5MgRjB8/HgEBAVAoFNi7d69BvyAIWLp0KVQqFdzc3BAdHY2CggKLcTds2IBu3brB1dUVEREROHXqlOjcWDAQEREZEQTpmhgajQZhYWHYsGGD2f7Vq1dj3bp12LRpE06ePAl3d3fExsaivr6+yZi7du1CUlISUlNTkZubi7CwMMTGxuLq1auicrPJlIRWq4VWqzU4JggCFAoOARERke1JuYbB3HeeUqmEUqk0OTcuLg5xcXFm4wiCgLfffhuvvPIKJkyYAAD48MMP4efnh71792Lq1Klmr3vrrbcwe/ZsPPPMMwCATZs24fPPP8fmzZvx8ssv3/WfQ/QIw/r16zF9+nTs3LkTAPDRRx/hwQcfREhICP7yl7/g9u3bFmOo1Wp4eXkZNEFXKzYVIiIiWQiCQrJm7jtPrVaLzqmoqAjl5eWIjo7WH/Py8kJERASys7PNXnPr1i3k5OQYXOPg4IDo6Ogmr2mKqBGG1157DatXr0ZMTAwWLFiA4uJirFmzBgsWLICDgwPWrl0LZ2dnLF++vNk4KSkpSEpKMjjWvmOIqMSJiIhaA3PfeeZGFywpLy8HAPj5+Rkc9/Pz0/cZq6ysRGNjo9lrfvzxR1HvL6pgSEtLQ1paGh5//HH897//RXh4OLZu3Ypp06YBAEJCQvDSSy9ZLBjMDcVwOoKIiOyFlFMSTU0/tDaipiRKS0sxePBgAEBYWBgcHBwwYMAAff+gQYNQWloqaYJERET3miBhk4q/vz8AoKKiwuB4RUWFvs+Yj48PHB0dRV3TFFEFg7+/P77//nsAQEFBARobG/WvAeC7776Dr6+vqASIiIjIsu7du8Pf3x+ZmZn6YzU1NTh58iQiIyPNXuPi4oLw8HCDa3Q6HTIzM5u8pimipiSmTZuG6dOnY8KECcjMzMRLL72EhQsX4vr161AoFHj99dfx5JNPikqAiIjI3thqp8e6ujoUFhbqXxcVFSEvLw8dOnRA165dMX/+fLz22mvo2bMnunfvjiVLliAgIADx8fH6a0aPHo2JEydi7ty5AICkpCTMmDEDgwcPxtChQ/H2229Do9Ho75q4W6IKhuXLl8PNzQ3Z2dmYPXs2Xn75ZYSFheGll17CzZs3MX78eKxYsUJUAkRERPbGVjs9njlzBqNGjdK/vrNYcsaMGUhLS8NLL70EjUaD559/HlVVVRgxYgTS09Ph6uqqv+b8+fOorKzUv54yZQquXbuGpUuXory8HAMGDEB6errJQkhLFIJgH8/kcnLpbOsUiAAA3q7ussU+0aW7bLEB4KHLRbLFrqrXyBa7NUsKeFi22G+VHpEtNiDvv3W5/73cvnVF1vj/8ZdutHx4+b8ki2VLfJYEERGREZ2tE7BDLBiIiIiMCOCt/sb4LAkiIiKy6L4YYWjN83StlZw/c7nJ+XcaUnhOttgAULsxQbbYHnN2yBa7Ndt8I8fWKbQY//+raTq7WN1nX+6LgoGIiEgMHackTLBgICIiMsI1DKa4hoGIiIgs4ggDERGREd5WaYoFAxERkRFOSZjilAQRERFZxBEGIiIiI5ySMMWCgYiIyAgLBlOiC4aysjJs3LgRx44dQ1lZGRwcHBAUFIT4+HjMnDkTjo6OcuRJRERENiRqDcOZM2cQGhqKL774Ag0NDSgoKEB4eDjc3d2xcOFCPPzww6itrbUYR6vVoqamxqDZyUMziYiIIEAhWWsrRBUM8+fPx4IFC3DmzBkcPXoUaWlp+Omnn7Bz505cuHABN2/exCuvvGIxjlqthpeXl0ETdJYLDSIiontBp5CutRWiCobc3Fw8/fTT+td/+MMfkJubi4qKCrRv3x6rV6/Gv/5l+bnfKSkpqK6uNmgKBw/x2RMREdE9IWoNg6+vL8rKyhAUFAQAqKiowO3bt+Hp6QkA6NmzJ27cuGExjlKphFKpNDimULShMoyIiFo1PkvClKgRhvj4eLzwwgtIT09HVlYWpk2bhqioKLi5uQEA8vPz0blzZ1kSJSIiulcECVtbIWqE4bXXXkNZWRnGjx+PxsZGREZGYtu2bfp+hUIBtVoteZJERET3Em+rNCWqYGjXrh127dqF+vp63L59G+3atTPoj4mJkTQ5IiIisg8t2rjJ1dVV6jyIiIjsho7r6kxwp0ciIiIjbWntgVT48CkiIiKyiCMMRERERrjo0RQLBiIiIiNtaYdGqdwXBUNVvcbWKdx3WvPPPMhLJVvsC9VlssUGgMQVF2WLfcJ3iGyxH7p6WrbYcuvpId/eM6frf5ItNpFY90XBQEREJAZ3ejTFgoGIiMgI75IwxbskiIiIyCKOMBARERnhokdTLBiIiIiM8LZKUywYiIiIjHANg6kWFQy3bt3C3r17kZ2djfLycgCAv78/hg0bhgkTJsDFxUXSJImIiMi2RC96LCwsRGhoKGbMmIGzZ89Cp9NBp9Ph7NmzmD59Ovr06YPCwkI5ciUiIrondArpWlsheoRhzpw56NevH86ePQtPT0+DvpqaGkyfPh2JiYk4cOCAZEkSERHdS1zDYEr0CMN//vMfvPbaaybFAgB4enpixYoVOHr0aLMxtFotampqDJogcMaIiIjub926dYNCoTBpiYmJZs9PS0szOdfV1VWW3EQXDN7e3rh48WKT/RcvXoS3t3ezMdRqNby8vAyaoKsVmwoREZEsdBI2MU6fPo2ysjJ9y8jIAABMmjSpyWs8PT0NrikuLhb5rndH9JTEc889h+nTp2PJkiUYPXo0/Pz8AAAVFRXIzMzEa6+9hhdffLHZGCkpKUhKSjI41r5jiNhUiIiIZCHYaO1Bp06dDF6vXLkSPXr0QFRUVJPXKBQK+Pv7y52a+ILh1Vdfhbu7O9asWYM///nPUCj+91MVBAH+/v5YtGgRXnrppWZjKJVKKJVKg2N34hAREbUlWq0WWq3W4Ji570Fjt27dwrZt25CUlNTsd2RdXR0CAwOh0+kwaNAgvPHGG+jTp48kuf9Wi7aGXrRoEUpLS3H+/HkcO3YMx44dw/nz51FaWmqxWCAiIrJ3Uk5JmJuGV6vVFnPYu3cvqqqqMHPmzCbP6d27NzZv3oxPP/0U27Ztg06nw7Bhw3D58uWW/tGbpBAkXm146dIlpKamYvPmzaKuc3KR7xGxRGK05sdbTw+IlC32H2/fli12a3689ZBOvWSLffoaH2/dlNu3rsgaf/0DT0kWa3bhBy0aYYiNjYWLiws+++yzu36vhoYGhIaGIiEhAStWrGhRvk2R/OFTN27cwNatW6UOS0RE1CoplUp4enoaNEvFQnFxMQ4ePIjnnntO1Hs5Oztj4MCBsuyHJHoNw759+5rtv3DhQouTISIisge2vtF/y5Yt8PX1xbhx40Rd19jYiG+//RZjx46VPCfRBUN8fDwUCkWz+yZwASMREbVmttyhUafTYcuWLZgxYwacnAy/pqdPn47OnTvr10C8+uqreOihhxAcHIyqqiqsWbMGxcXFokcm7oboKQmVSoXdu3frt4Q2brm5uZInSUREdC/Zah8GADh48CBKSkowa9Ysk76SkhKUlf26Furnn3/G7NmzERoairFjx6KmpgbHjx/Hgw8+2IJ3bp7oEYbw8HDk5ORgwoQJZvstjT4QERFR02JiYpr8Hj106JDB67Vr12Lt2rX3IKsWFAzJycnQaDRN9gcHByMrK8uqpIiIiGyJz5IwJfltlS3F2yqJ7l+/lDb//BlruQWMlDU+3Xty31b5167S3Va5sGSbZLFsSfLbKomIiKjtET0lQURE1NbZ8i4Je8WCgYiIyAjXMJjilAQRERFZxBEGIiIiI3ZxN4CdkXyEoaKiAq+++qrUYYmIiO4ZHQTJWlshecFQXl6O5cuXSx2WiIiIbEj0lMQ333zTbH9+fn6LkyEiIrIHXPRoSnTBMGDAgCa3f75znA+fIiKi1qztTCRIR3TB0KFDB6xevRqjR4822//dd99h/PjxzcbQarXQarUGx1hoEBGRveAIg6kWPXyqtLQUgYGBZvurqqosPnxKrVabrHNQOLSDwtFTbDpERER0D4he9PjCCy+gW7duTfZ37doVW7ZsaTZGSkoKqqurDZrCwUNsKkRERLLQKaRrbYXoEYaJEyc229++fXvMmDGj2XOUSiWUSqXBMU5HEBGRvWhLt0NKRfLbKi9duoRZs2ZJHZaIiIhsSPKC4caNG9i6davUYYmIiO4ZQcLWVoiekti3b1+z/RcuXGhxMkRERPaAd0mYEl0wxMfHN7kPwx1cj0BERNS2iJ6SUKlU2L17N3Q6ndmWm5srR55ERET3DJ8lYUp0wRAeHo6cnJwm+y2NPhAREdk7rmEwJXpKIjk5GRqNpsn+4OBgZGVlWZUUERER2RfRBcPIkSOb7Xd3d0dUVFSLEyIiIrI1Lno0JbpgIKL7U5CXSrbYbgHN/yJirSvDesoW+7kL7WSL/WX5WdliA/L+nV6oLpMt9r3QltYeSIUFAxERkRGWC6Yk37iJiIiI2h6OMBARERnhGgZTLBiIiIiMCJyUMMEpCSIiIrKoxQXD5cuXUVdXZ3K8oaEBR44csSopIiIiW9JJ2NoK0QVDWVkZhg4disDAQHh7e2P69OkGhcONGzcwatQoSZMkIiK6l7g1tCnRBcPLL78MBwcHnDx5Eunp6fj+++8xatQo/Pzzz/pzuDU0ERFR2yJ60ePBgwexZ88eDB48GADwn//8B5MmTcKjjz6KzMxMAJafVqnVaqHVag2OCYLAp1wSEZFd4K+9pkSPMFRXV6N9+/b610qlErt370a3bt0watQoXL161WIMtVoNLy8vgyboasWmQkREJAtOSZgSXTAEBQXhm2++MTjm5OSETz75BEFBQfj9739vMUZKSgqqq6sNmsLBQ2wqREREdI+ILhji4uLw3nvvmRy/UzQMGDDA4hoGpVIJT09Pg8bpCCIishe2ukti2bJlUCgUBi0kJKTZaz755BOEhITA1dUV/fr1wxdffCHyXe+O6DUMr7/+Om7evGk+mJMT/v3vf+PKlStWJ0ZERGQrtty4qU+fPjh48KD+tZNT01/Vx48fR0JCAtRqNX7/+99j+/btiI+PR25uLvr27StpXqJHGJycnODp6dlkf1lZGZYvX25VUkRERLZky30YnJyc4O/vr28+Pj5NnvvOO+9gzJgxSE5ORmhoKFasWIFBgwZh/fr1LXjn5km+0+ONGzewdetWqcMSERG1SlqtFjU1NQbN+E7B3yooKEBAQACCgoIwbdo0lJSUNHludnY2oqOjDY7FxsYiOztbsvzvED0lsW/fvmb7L1y40OJkiIiI7IGUUxJqtdpk5D01NRXLli0zOTciIgJpaWno3bu3fsR+5MiROHfuHDw8TG8OKC8vh5+fn8ExPz8/lJeXS5b/HaILhvj4eCgUimYXNnIBIxERtWZSbumckpKCpKQkg2NKpdLsuXFxcfr/7t+/PyIiIhAYGIiPP/4Yzz77rIRZiSd6SkKlUmH37t3Q6XRmW25urhx5EhERtUrm7gxsqmAw5u3tjV69eqGwsNBsv7+/PyoqKgyOVVRUwN/f3+q8jYkuGMLDw5GTk9Nkv6XRByIiInunEwTJmjXq6upw/vx5qFQqs/2RkZH6XZbvyMjIQGRkpFXva47oKYnk5GRoNJom+4ODg5GVlWVVUkRERLZkq197Fy5ciPHjxyMwMBClpaVITU2Fo6MjEhISAADTp09H586doVarAQDz5s1DVFQU3nzzTYwbNw47d+7EmTNnzO6XZC3RBcPIkSOb7Xd3d0dUVFSLE6J7x9vVXbbYVfVNF5UknyAv87+FSOFCdZlsseXW+XiBbLFrNybIFttjzlnZYgPADW2NrPFJvMuXLyMhIQHXr19Hp06dMGLECJw4cQKdOnUCAJSUlMDB4dfJgWHDhmH79u145ZVX8Je//AU9e/bE3r17Jd+DAWhBwUBERNTW2eoZEDt37my2/9ChQybHJk2ahEmTJsmU0a9YMBARERmx5U6P9kryjZuIiIio7eEIAxERkREp92FoK1gwEBERGbHVGgZ7xoKBiIjICNcwmGpRwXD9+nV88803CAsLQ4cOHVBZWYkPPvgAWq0WkyZNQmhoqNR5EhERkQ2JLhhOnTqFmJgY1NTUwNvbGxkZGZg0aRKcnJyg0+mwcuVKHDt2DIMGDZIjXyIiItlxDYMp0XdJLF68GJMmTUJ1dTX+8pe/ID4+HqNHj8ZPP/2EwsJCTJ06FStWrJAjVyIiontCEATJWlshumDIyclBUlISPDw8MG/ePJSWlmL27Nn6/rlz5+L06dPNxjD3bPC29EMlIiJqa0QXDLdu3YKbmxsAwNnZGb/73e/g4+Oj7/fx8cH169ebjaFWq+Hl5WXQBF2t2FSIiIhkoYMgWWsrRBcMDzzwAC5cuKB/vXPnToOnaJWVlRkUEOakpKSgurraoCkcPMSmQkREJAudhK2tEL3ocerUqbh69ar+9bhx4wz69+3bh6FDhzYbQ6lUmjwLXKFQiE2FiIiI7hHRBUNqamqz/YsXL4ajo2OLEyIiIrI17sNgSvJnSVy/fh1z5syROiwREdE9wzUMpiQvGG7cuIGtW7dKHZaIiIhsSPSUxL59+5rt/+2CSCIiotaIt/qbEl0wxMfHQ6FQNPvD5AJGIiJqzdrS3Q1SET0loVKpsHv3buh0OrMtNzdXjjyJiIjuGUHC/7UVoguG8PBw5OTkNNlvafSBiIiIWh/RUxLJycnQaDRN9gcHByMrK8uqpIiIiGypLd3dIBWFYCfDAU4unWWL7e3qLlvsqvqmiyciavumB0TKFnvd47dkiw0A3uubHi22d7dvXZE1/uguMZLFyrz8lWSxbEny2yqJiIio7RE9JUFERNTWcUrCFAsGIiIiI23p7gapcEqCiIiILOIIAxERkRGdfdwPYFdYMBARERlhuWBKsimJoKAgFBQUSBWOiIiI7IjoEYZ169aZPV5SUoItW7bA398fAPCnP/3JusyIiIhshHdJmBJdMMyfPx+dO3eGk5PhpTqdDh9++CGcnZ2hUChYMBARUavFgsGU6ILh+eefx8mTJ7F9+3aEhobqjzs7O+Orr77Cgw8+aDGGVquFVqs1OCYIAp9ySUREdsFONkG2K6LXMGzatAlLly5FbGws1q9f36I3VavV8PLyMmiCrrZFsYiIiEh+LVr0OHHiRGRnZ2PPnj2Ii4tDeXm5qOtTUlJQXV1t0BQOHi1JhYiISHI6CJK1tqLFt1V27twZBw8exMqVKzFw4EBRwzdKpRJKpdLgGKcjiIjIXnCnR1NW7cOgUCiQkpKCmJgYHDt2DCqVSqq8iIiIyI5Isg9DeHg45s2bh/bt2+PSpUuYNWuWFGGJiIhsQhAEyZoYarUaQ4YMgYeHB3x9fREfH4/8/Pxmr0lLS4NCoTBorq6u1vzxzZL8WRI3btzA1q1bpQ5LRER0z9hqDcPhw4eRmJiIEydOICMjAw0NDYiJiYFGo2n2Ok9PT5SVlelbcXGxNX98s0RPSezbt6/Z/gsXLrQ4GSIiovtZenq6weu0tDT4+voiJycHDz/8cJPXKRQK/caJchFdMMTHx0OhUDQ7zMIFjERE1JpJuQ+Dub2HzC3+N6e6uhoA0KFDh2bPq6urQ2BgIHQ6HQYNGoQ33ngDffr0aXnSZoieklCpVNi9ezd0Op3ZlpubK2mCRERE95qUUxLm9h5Sq9WWc9DpMH/+fAwfPhx9+/Zt8rzevXtj8+bN+PTTT7Ft2zbodDoMGzYMly9flvJHIn6EITw8HDk5OZgwYYLZfkujD0RERPeTlJQUJCUlGRy7m9GFxMREnDt3DseOHWv2vMjISERGRupfDxs2DKGhofj73/+OFStWtCxpM0QXDMnJyc0uvggODkZWVpZVSREREdmSlPsw3O30w2/NnTsX+/fvx5EjR9ClSxdR1zo7O2PgwIEoLCwUdZ0loguGkSNHNtvv7u6OqKioFickh6r65leX2jNvV3fZYrfmn4uc+DMnMY5pLsoW23t9mWyxAaB2Y4JssT3m7JAt9r2gs9FIuSAIePHFF7Fnzx4cOnQI3bt3Fx2jsbER3377LcaOHStpblZt3ERERNQW2Wqnx8TERGzfvh2ffvopPDw89I9e8PLygpubGwBg+vTp6Ny5s34dxKuvvoqHHnoIwcHBqKqqwpo1a1BcXIznnntO0txYMBAREdmJjRs3AgAeeeQRg+NbtmzBzJkzAQAlJSVwcPj1noWff/4Zs2fPRnl5Odq3b4/w8HAcP378rp4eLQYLBiIiIiO2nJKw5NChQwav165di7Vr18qU0a9YMBARERnhw6dMSb41NBEREbU9Vo8wCIKAQ4cOobCwECqVCrGxsXB2dpYiNyIiIpuw1ZSEPRNdMIwdOxY7duyAl5cXbty4gbFjx+LUqVPw8fHB9evX0atXLxw5cgSdOnWSI18iIiLZcUrClOgpifT0dP2e2K+88gpqa2tx/vx5XL16FcXFxXB3d8fSpUslT5SIiIhsx6o1DF9//TXUarV+Y4kuXbpg1apVOHDggCTJERER2YJOECRrbUWL1jDceRrlzz//jB49ehj0BQcHo7S0tNnrzT25SxAEPuWSiIjsAqckTLVohGHmzJl4/PHH0dDQgKKiIoO+8vJyeHt7N3u9uSd3CbralqRCRERE94DogmHGjBnw9fWFl5cXJkyYgJs3bxr0//vf/8aAAQOajZGSkoLq6mqDpnDwEJsKERGRLARBJ1lrK0RPSWzZsqXZ/tTUVDg6OjZ7jrknd3E6goiI7IWOUxImJN+46caNG/jjH/8odVgiIqJ7RhAEyVpbIUvBsHXrVqnDEhERkQ2JnpLYt29fs/0XLlxocTJERET2gFMSpkQXDPHx8VAoFM0Os3A9AhERtWZtaSpBKqKnJFQqFXbv3g2dTme25ebmypEnERER2ZDogiE8PBw5OTlN9lsafSAiIrJ33OnRlOgpieTkZGg0mib7g4ODkZWVZVVSREREtsSdHk2JLhhGjhzZbL+7uzuioqJanBARERHZnxY9S4Lunar6pkdz7Jm3q7us8Vvrz0Vucv7c+TM3r7ebv2yxL1SXyRYbADzm7JAtduUTvWSLfS9wat0UCwYiIiIjvK3SlOQbNxEREVHbwxEGIiIiI5ySMMWCgYiIyEhbuh1SKiwYiIiIjHCEwZToNQyXL19GZWWl/vXRo0cxbdo0jBw5Ek899RSys7MlTZCIiIhsT3TB8MQTT+DEiRMAgE8//RSPPPII6urqMHz4cNy8eRNRUVHYv3+/5IkSERHdKzoIkrW2QvSUxHfffYc+ffoAANRqNd544w0sWrRI379+/XosXboUv//976XLkoiI6B7ilIQp0SMMTk5OqK2tBQAUFRUhLi7OoD8uLg75+fnNxtBqtaipqTFo/MshIiKyX6ILhqioKOzY8b/dwQYOHIhDhw4Z9GdlZaFz587NxlCr1fDy8jJogq5WbCpERESy4MOnTImekli5ciVGjhyJ0tJSjBgxAosXL8bp06cRGhqK/Px87Nq1C5s2bWo2RkpKCpKSkgyOte8YIjYVIiIiWfDhU6ZEFwyhoaE4efIkXnnlFaxevRoajQb//Oc/4eTkhCFDhmDnzp2Ij49vNoZSqYRSqTQ4plAoxKZCRERE90iL9mHo0aMHduzYAUEQcPXqVeh0Ovj4+MDZ2Vnq/IiIiO65tjSVIBWrniWhUCjg5+cHlUqlLxYuXbqEWbNmSZIcERGRLQiCIFlrKyR/+NSNGzewdetWqcMSERGRDYmekti3b1+z/RcuXGhxMkRERPaAix5NiS4Y4uPjoVAomh1m4QJGIiJqzdrSVIJURE9JqFQq7N69GzqdzmzLzc2VI08iIqJ7xpZrGDZs2IBu3brB1dUVEREROHXqVLPnf/LJJwgJCYGrqyv69euHL774oqV/7GaJLhjCw8ORk5PTZL+l0QciIiIyb9euXUhKSkJqaipyc3MRFhaG2NhYXL161ez5x48fR0JCAp599lmcPXsW8fHxiI+Px7lz5yTPTSGI/HY/evQoNBoNxowZY7Zfo9HgzJkziIqKEpWIk0vzu0NS6+Lt6i5r/Kp6jWyx5cxdzryB1p17axXnP1C22F+Wn5Utttwqn+gla3zvHVmyxpfyO0lTewFardbgmLn9iAAgIiICQ4YMwfr16wEAOp0ODzzwAF588UW8/PLLJudPmTIFGo3G4KGPDz30EAYMGGBxE0XRhFamvr5eSE1NFerr61td/NYaW+74rTW23PFba2y547fW2HLHZ+72KzU1VQBg0FJTU03O02q1gqOjo7Bnzx6D49OnTxcee+wxs7EfeOABYe3atQbHli5dKvTv31+i7H/V6gqG6upqAYBQXV3d6uK31thyx2+tseWO31pjyx2/tcaWOz5zt1/19fVCdXW1QTNXHF25ckUAIBw/ftzgeHJysjB06FCzsZ2dnYXt27cbHNuwYYPg6+sr3R/g/9einR6JiIjo7jQ1/dDaSL5xExEREYnn4+MDR0dHVFRUGByvqKiAv7+/2Wv8/f1FnW8NFgxERER2wMXFBeHh4cjMzNQf0+l0yMzMRGRkpNlrIiMjDc4HgIyMjCbPt0arm5JQKpVITU2VbXhHzvitNbbc8VtrbLnjt9bYcsdvrbHljs/c24akpCTMmDEDgwcPxtChQ/H2229Do9HgmWeeAQBMnz4dnTt3hlqtBgDMmzcPUVFRePPNNzFu3Djs3LkTZ86cwXvvvSd5bqJvqyQiIiL5rF+/HmvWrEF5eTkGDBiAdevWISIiAgDwyCOPoFu3bkhLS9Of/8knn+CVV17BxYsX0bNnT6xevRpjx46VPC8WDERERGQR1zAQERGRRSwYiIiIyCIWDERERGQRCwYiIiKyqNUVDGIf+3m3jhw5gvHjxyMgIAAKhQJ79+6VJC4AqNVqDBkyBB4eHvD19UV8fDzy8/Mlib1x40b0798fnp6e8PT0RGRkJL788ktJYhtbuXIlFAoF5s+fL0m8ZcuWQaFQGLSQkBBJYgPAlStX8NRTT6Fjx45wc3NDv379cObMGUlid+vWzSR3hUKBxMREq2M3NjZiyZIl6N69O9zc3NCjRw+sWLFCsqfA1tbWYv78+QgMDISbmxuGDRuG06dPtyiWpc+NIAhYunQpVCoV3NzcEB0djYKCAkli7969GzExMejYsSMUCgXy8vIkybuhoQGLFi1Cv3794O7ujoCAAEyfPh2lpaWSxAf+928/JCQE7u7uaN++PaKjo3Hy5ElJYv/WCy+8AIVCgbfffluS2DNnzjT5N9/UgwhbmvsPP/yAxx57DF5eXnB3d8eQIUNQUlJy1+9B8mlVBYPYx36KodFoEBYWhg0bNkiQqaHDhw8jMTERJ06cQEZGBhoaGhATEwONxvqn/3Xp0gUrV65ETk4Ozpw5g0cffRQTJkzAd999J0Hmvzp9+jT+/ve/o3///pLG7dOnD8rKyvTt2LFjksT9+eefMXz4cDg7O+PLL7/E999/jzfffBPt27eXJP7p06cN8s7IyAAATJo0yerYq1atwsaNG7F+/Xr88MMPWLVqFVavXo2//e1vVscGgOeeew4ZGRn46KOP8O233yImJgbR0dG4cuWK6FiWPjerV6/GunXrsGnTJpw8eRLu7u6IjY1FfX291bE1Gg1GjBiBVatWSZr3zZs3kZubiyVLliA3Nxe7d+9Gfn4+HnvsMUniA0CvXr2wfv16fPvttzh27Bi6deuGmJgYXLt2zerYd+zZswcnTpxAQECAZHkDwJgxYwz+7e/YsUOy+OfPn8eIESMQEhKCQ4cO4ZtvvsGSJUvg6up61+9BMpL86RQyGjp0qJCYmKh/3djYKAQEBAhqtVrS9wFg8rQwKV29elUAIBw+fFiW+O3btxf+8Y9/SBavtrZW6Nmzp5CRkSFERUUJ8+bNkyRuamqqEBYWJkksY4sWLRJGjBghS2xz5s2bJ/To0UPQ6XRWxxo3bpwwa9Ysg2OPP/64MG3aNKtj37x5U3B0dBT2799vcHzQoEHC4sWLrYpt/LnR6XSCv7+/sGbNGv2xqqoqQalUCjt27LAq9m8VFRUJAISzZ8+2IOu7+7yfOnVKACAUFxfLEv/Ow5cOHjwoSezLly8LnTt3Fs6dOycEBgaaPM2wpbFnzJghTJgwQXSsu40/ZcoU4amnnpIkPkmv1Yww3Lp1Czk5OYiOjtYfc3BwQHR0NLKzs22YmXjV1dUAgA4dOkgat7GxETt37oRGo5F0W9DExESMGzfO4GcvlYKCAgQEBCAoKAjTpk2TbOhx3759GDx4MCZNmgRfX18MHDgQ77//viSxjd26dQvbtm3DrFmzoFAorI43bNgwZGZm4qeffgIA/Pe//8WxY8cQFxdndezbt2+jsbHR5Dc2Nzc3yUZ37igqKkJ5ebnBvxsvLy9ERES0ys+sQqGAt7e35LFv3bqF9957D15eXggLC7M6nk6nw9NPP43k5GT06dNHggwNHTp0CL6+vujduzfmzJmD69evSxJXp9Ph888/R69evRAbGwtfX19ERERIOj1M1mk1BUNlZSUaGxvh5+dncNzPzw/l5eU2yko8nU6H+fPnY/jw4ejbt68kMb/99lu0a9cOSqUSL7zwAvbs2YMHH3xQktg7d+5Ebm6ufhtSKUVERCAtLQ3p6enYuHEjioqKMHLkSNTW1lod+8KFC9i4cSN69uyJAwcOYM6cOfjTn/6ErVu3SpC5ob1796KqqgozZ86UJN7LL7+MqVOnIiQkBM7Ozhg4cCDmz5+PadOmWR3bw8MDkZGRWLFiBUpLS9HY2Iht27YhOzsbZWVlEmT/qzufy9b+ma2vr8eiRYuQkJAAT09PyeLu378f7dq1g6urK9auXYuMjAz4+PhYHXfVqlVwcnLCn/70JwmyNDRmzBh8+OGHyMzMxKpVq3D48GHExcWhsbHR6thXr15FXV0dVq5ciTFjxuCrr77CxIkT8fjjj+Pw4cMSZE/WanXPkmjtEhMTce7cOUl/m+vduzfy8vJQXV2Nf/3rX5gxYwYOHz5sddFw6dIlzJs3DxkZGbLMIf72N+b+/fsjIiICgYGB+Pjjj/Hss89aFVun02Hw4MF44403AAADBw7EuXPnsGnTJsyYMcOq2MY++OADxMXFiZorbs7HH3+Mf/7zn9i+fTv69OmDvLw8zJ8/HwEBAZLk/tFHH2HWrFno3LkzHB0dMWjQICQkJCAnJ0eC7NuWhoYGTJ48GYIgYOPGjZLGHjVqFPLy8lBZWYn3338fkydPxsmTJ+Hr69vimDk5OXjnnXeQm5sryWiXsalTp+r/u1+/fujfvz969OiBQ4cOYfTo0VbF1ul0AIAJEyZgwYIFAIABAwbg+PHj2LRpE6KioqyKT9ZrNSMMLXnsp72ZO3cu9u/fj6ysLHTp0kWyuC4uLggODkZ4eDjUajXCwsLwzjvvWB03JycHV69exaBBg+Dk5AQnJyccPnwY69atg5OTkyS/VfyWt7c3evXqhcLCQqtjqVQqk4IpNDRU8tXWxcXFOHjwIJ577jnJYiYnJ+tHGfr164enn34aCxYskGyUp0ePHjh8+DDq6upw6dIlnDp1Cg0NDQgKCpIk/h13Ppet9TN7p1goLi5GRkaGpKMLAODu7o7g4GA89NBD+OCDD+Dk5IQPPvjAqphHjx7F1atX0bVrV/1ntri4GH/+85/RrVs3aRL/jaCgIPj4+EjymfXx8YGTk9M9+dxSy7SagqElj/20F4IgYO7cudizZw++/vprdO/eXdb30+l00Gq1VscZPXo0vv32W+Tl5enb4MGDMW3aNOTl5cHR0VGCbH9VV1eH8+fPQ6VSWR1r+PDhJreu/vTTTwgMDLQ69m9t2bIFvr6+GDdunGQxb968CQcHw4+mo6Oj/jcwqbi7u0OlUuHnn3/GgQMHMGHCBEnjd+/eHf7+/gaf2ZqaGpw8edLuP7N3ioWCggIcPHgQHTt2lP09pfjcPv300/jmm28MPrMBAQFITk7GgQMHJMr0V5cvX8b169cl+cy6uLhgyJAh9+RzSy3TqqYkLD320xp1dXUGVXJRURHy8vLQoUMHdO3a1arYiYmJ2L59Oz799FN4eHjo52+9vLzg5uZmVeyUlBTExcWha9euqK2txfbt23Ho0CFJ/s/Bw8PDZJ2Fu7s7OnbsKMn6i4ULF2L8+PEIDAxEaWkpUlNT4ejoiISEBKtjL1iwAMOGDcMbb7yByZMn49SpU3jvvfckfeSrTqfDli1bMGPGDDg5SfdRGj9+PF5//XV07doVffr0wdmzZ/HWW29h1qxZksQ/cOAABEFA7969UVhYiOTkZISEhLToc2TpczN//ny89tpr6NmzJ7p3744lS5YgICAA8fHxVse+ceMGSkpK9Psj3Pmi8ff3tziC0VxslUqFJ598Erm5udi/fz8aGxv1n9kOHTrAxcXFqtw7duyI119/HY899hhUKhUqKyuxYcMGXLly5a5uy7X0czEubpydneHv74/evXtbFbtDhw5Yvnw5nnjiCfj7++P8+fN46aWXEBwcjNjYWIux7yb35ORkTJkyBQ8//DBGjRqF9PR0fPbZZzh06NBdxSeZ2fguDdH+9re/CV27dhVcXFyEoUOHCidOnJAkblZWlgDApM2YMcPq2ObiAhC2bNlidexZs2YJgYGBgouLi9CpUydh9OjRwldffWV13KZIeVvllClTBJVKJbi4uAidO3cWpkyZIhQWFkoSWxAE4bPPPhP69u0rKJVKISQkRHjvvfckiy0IgnDgwAEBgJCfny9p3JqaGmHevHlC165dBVdXVyEoKEhYvHixoNVqJYm/a9cuISgoSHBxcRH8/f2FxMREoaqqqkWxLH1udDqdsGTJEsHPz09QKpXC6NGj7/rnZSn2li1bzPanpqZaFfvObZrmWlZWltW5//LLL8LEiROFgIAAwcXFRVCpVMJjjz0mnDp1SpKfizExt1U2F/vmzZtCTEyM0KlTJ8HZ2VkIDAwUZs+eLZSXl99V7LvN/YMPPhCCg4MFV1dXISwsTNi7d+9dxyd58fHWREREZFGrWcNAREREtsOCgYiIiCxiwUBEREQWsWAgIiIii1gwEBERkUUsGIiIiMgiFgxERERkEQsGIiIisogFAxEREVnEgoGIiIgsYsFAREREFv1/OoOC60rphvwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the Precision, Recall, F1 scores of each classes\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import pandas as pd\n",
        "scores = pd.DataFrame(precision_recall_fscore_support(cnnPredicted.to('cpu'), labels.to('cpu'), labels = list(range(18))),index=['precision','recall','f1','support'], columns = list(label_dict)[:18])\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "1e7wf_Ai0FH4",
        "outputId": "42366589-b803-42a1-b500-f785e330112f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Ambient      Blues   Children  Classical    Country  Electronic  \\\n",
              "precision   0.800000   0.750000   0.650000   1.000000   0.500000    0.100000   \n",
              "recall      1.000000   0.576923   1.000000   0.800000   0.909091    0.500000   \n",
              "f1          0.888889   0.652174   0.787879   0.888889   0.645161    0.166667   \n",
              "support    16.000000  26.000000  13.000000  25.000000  11.000000    4.000000   \n",
              "\n",
              "                Folk       Jazz      Latin        Pop        Rap  Reggae  \\\n",
              "precision   0.550000   0.550000   0.550000   0.800000   0.500000     1.0   \n",
              "recall      0.916667   0.611111   1.000000   0.571429   0.769231     1.0   \n",
              "f1          0.687500   0.578947   0.709677   0.666667   0.606061     1.0   \n",
              "support    12.000000  18.000000  11.000000  28.000000  13.000000    20.0   \n",
              "\n",
              "           Religious       Rock       Soul  Soundtracks  Unknown      World  \n",
              "precision   0.600000   0.800000   0.700000     0.800000      0.0   0.650000  \n",
              "recall      0.923077   0.231884   1.000000     0.551724      0.0   0.722222  \n",
              "f1          0.727273   0.359551   0.823529     0.653061      0.0   0.684211  \n",
              "support    13.000000  69.000000  14.000000    29.000000      0.0  18.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0922fa1f-5bff-4adf-9161-801951bf086f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ambient</th>\n",
              "      <th>Blues</th>\n",
              "      <th>Children</th>\n",
              "      <th>Classical</th>\n",
              "      <th>Country</th>\n",
              "      <th>Electronic</th>\n",
              "      <th>Folk</th>\n",
              "      <th>Jazz</th>\n",
              "      <th>Latin</th>\n",
              "      <th>Pop</th>\n",
              "      <th>Rap</th>\n",
              "      <th>Reggae</th>\n",
              "      <th>Religious</th>\n",
              "      <th>Rock</th>\n",
              "      <th>Soul</th>\n",
              "      <th>Soundtracks</th>\n",
              "      <th>Unknown</th>\n",
              "      <th>World</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.231884</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.722222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1</th>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.652174</td>\n",
              "      <td>0.787879</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.645161</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>0.709677</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.606061</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.359551</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.653061</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.684211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>16.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>20.0</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0922fa1f-5bff-4adf-9161-801951bf086f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0922fa1f-5bff-4adf-9161-801951bf086f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0922fa1f-5bff-4adf-9161-801951bf086f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0e770876-f8a8-451d-aa58-7c0ba231a55f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0e770876-f8a8-451d-aa58-7c0ba231a55f')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0e770876-f8a8-451d-aa58-7c0ba231a55f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_e60a0b28-4313-4653-a028-938274a14ef1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('scores')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e60a0b28-4313-4653-a028-938274a14ef1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('scores');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the label_dict\n",
        "label_dict = {}\n",
        "for cl in le.classes_:\n",
        "    label_dict.update({cl:le.transform([cl])[0]})\n",
        "label_dict"
      ],
      "metadata": {
        "id": "CtA905R9OigX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}