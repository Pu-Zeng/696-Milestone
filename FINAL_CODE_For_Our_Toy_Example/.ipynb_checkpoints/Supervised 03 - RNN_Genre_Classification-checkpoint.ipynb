{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1698093621580,
     "user": {
      "displayName": "Pu Zeng",
      "userId": "06309785996409442047"
     },
     "user_tz": 300
    },
    "id": "UMzLgypdrY3v"
   },
   "outputs": [],
   "source": [
    "# Train the RNN Genre Classification Model#\n",
    "# Last editted by Pu Zeng, 18/10/2023 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2I3kQN9aMnp"
   },
   "source": [
    "# Install library needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3662,
     "status": "ok",
     "timestamp": 1698093625240,
     "user": {
      "displayName": "Pu Zeng",
      "userId": "06309785996409442047"
     },
     "user_tz": 300
    },
    "id": "6vrNcyOjaHjT",
    "outputId": "07167d94-d906-4546-9af5-839f64d99b66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '696-Milestone'...\n",
      "remote: Enumerating objects: 3059, done.\u001b[K\n",
      "remote: Counting objects: 100% (3059/3059), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2559/2559), done.\u001b[K\n",
      "remote: Total 3059 (delta 62), reused 3039 (delta 42), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (3059/3059), 34.84 MiB | 24.45 MiB/s, done.\n",
      "Resolving deltas: 100% (62/62), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Pu-Zeng/696-Milestone.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22430,
     "status": "ok",
     "timestamp": 1698093647668,
     "user": {
      "displayName": "Pu Zeng",
      "userId": "06309785996409442047"
     },
     "user_tz": 300
    },
    "id": "48GApgifaPCU",
    "outputId": "c20bcb28-1e89-41d9-bfc7-a9b7e6741654"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pretty_midi\n",
      "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.23.5)\n",
      "Collecting mido>=1.1.16 (from pretty_midi)\n",
      "  Downloading mido-1.3.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.16.0)\n",
      "Requirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.10/dist-packages (from mido>=1.1.16->pretty_midi) (23.2)\n",
      "Building wheels for collected packages: pretty_midi\n",
      "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592287 sha256=b5bd9b62052ef904fbbd2b279c1d4af0d3d7742f1340b952a1c777fde2ce9fa3\n",
      "  Stored in directory: /root/.cache/pip/wheels/cd/a5/30/7b8b7f58709f5150f67f98fde4b891ebf0be9ef07a8af49f25\n",
      "Successfully built pretty_midi\n",
      "Installing collected packages: mido, pretty_midi\n",
      "Successfully installed mido-1.3.0 pretty_midi-0.2.10\n",
      "Collecting pypianoroll\n",
      "  Downloading pypianoroll-1.0.4-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pypianoroll) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pypianoroll) (1.11.3)\n",
      "Requirement already satisfied: pretty-midi>=0.2.8 in /usr/local/lib/python3.10/dist-packages (from pypianoroll) (0.2.10)\n",
      "Requirement already satisfied: matplotlib>=1.5 in /usr/local/lib/python3.10/dist-packages (from pypianoroll) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pypianoroll) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pypianoroll) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pypianoroll) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pypianoroll) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pypianoroll) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pypianoroll) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pypianoroll) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pypianoroll) (2.8.2)\n",
      "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.10/dist-packages (from pretty-midi>=0.2.8->pypianoroll) (1.3.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pretty-midi>=0.2.8->pypianoroll) (1.16.0)\n",
      "Installing collected packages: pypianoroll\n",
      "Successfully installed pypianoroll-1.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pretty_midi\n",
    "!pip install pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9937,
     "status": "ok",
     "timestamp": 1698093657602,
     "user": {
      "displayName": "Pu Zeng",
      "userId": "06309785996409442047"
     },
     "user_tz": 300
    },
    "id": "u8eSS9tg62ge"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# import mido\n",
    "import string\n",
    "import numpy as np\n",
    "# from utilis import get_pianoroll_data\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #Define device\n",
    "input_dir = '../Toy_Dataset/adl-piano-midi/' #Please change\n",
    "output_dir = './Processed_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63D2LyMNaT1j",
    "outputId": "c0817352-5a71-4b7e-a3c0-c0981f0fe049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All content numbers is 18\n",
      "145it [01:11,  2.02it/s]\n",
      "146it [00:57,  2.52it/s]\n",
      "146it [01:04,  2.25it/s]\n",
      "146it [01:08,  2.14it/s]\n",
      "145it [00:54,  2.67it/s]\n",
      "146it [01:00,  2.41it/s]\n",
      "146it [01:19,  1.85it/s]\n",
      "146it [01:16,  1.90it/s]\n",
      "27it [00:09,  3.06it/s]"
     ]
    }
   ],
   "source": [
    "# Use our python script to preprocess the midi files into matrices\n",
    "!mkdir Processed_Data\n",
    "!python /content/696-Milestone/Data_Preprocessing/Piano_Genre_Classification.py /content/696-Milestone/Toy_Dataset/adl-piano-midi/ ./Processed_Data/\n",
    "# It takes some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAef7ynBagQr"
   },
   "outputs": [],
   "source": [
    "# Pick the midi files with more than 51.2s in the middle\n",
    "\n",
    "from tqdm import tqdm\n",
    "X = []\n",
    "y = []\n",
    "for i in tqdm(range(1,10)):\n",
    "    file=open(output_dir+\"music_data\"+str(i)+\".bin\",\"rb\")\n",
    "    music_data = pickle.load(file) #保存list到文件\n",
    "    file.close()\n",
    "    for m in music_data:\n",
    "        if m[2].shape[0]>=1012:\n",
    "            X.append(m[2][500:1012,:])\n",
    "            y.append(m[0])\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6LAXkrKag75"
   },
   "outputs": [],
   "source": [
    "# The dataset is just a toy dataset, so there are far less labels than the whole Lakh dataset, we need to drop those will few samples\n",
    "X_new = []\n",
    "y_new = []\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "temp_y = np.array(y)\n",
    "count_y_dict = {}\n",
    "for i in np.unique(temp_y):\n",
    "  if (temp_y==i).sum()>=3:\n",
    "    X_new = X_new + list(X[temp_y==i])\n",
    "    y_new = y_new + list(y[temp_y==i])\n",
    "X = np.array(X_new)\n",
    "y = np.array(y_new)\n",
    "num_classes = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twiggB0BmI2a"
   },
   "outputs": [],
   "source": [
    "# Transform labels into one-hot variables\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "label_dict = {}\n",
    "for cl in le.classes_:\n",
    "    label_dict.update({cl:le.transform([cl])[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuCdyJovauK0"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p80pW8l5HiMo"
   },
   "outputs": [],
   "source": [
    "# RNN Model\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x => (batch_size, seq_len, hidden_size)\n",
    "        scores = self.v(torch.tanh(self.W(x))) # (batch_size, seq_len, 1)\n",
    "        attention_weights = F.softmax(scores, dim=1)\n",
    "        context_vector = attention_weights * x\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        # self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, 2, dropout=0.3, batch_first=True, bidirectional=True)\n",
    "        self.attention = Attention(hidden_size * 2)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        context_vector, attention_weights = self.attention(lstm_out)\n",
    "        output = self.fc(context_vector)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# cnn = TextClassifier(128, embedding_dim=128, hidden_size=1024, num_classes=17).to('cuda')\n",
    "\n",
    "hiddenSize = 300\n",
    "# Number of feature filters in second convolutional layer\n",
    "numFilters = 25\n",
    "# Dropout rate\n",
    "dropoutRate = 0.3\n",
    "# Activation function\n",
    "activation = \"ReLU\"\n",
    "# Learning rate\n",
    "learningRate = 0.01\n",
    "# Momentum for SGD optimizer\n",
    "momentum = 0.9\n",
    "# Number of training epochs\n",
    "numEpochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rds7kBErIyiO"
   },
   "outputs": [],
   "source": [
    "# Train and Validation function\n",
    "\n",
    "from tqdm import tqdm\n",
    "def train_epoch(cnn,device,dataloader,loss_fn,optimizer):\n",
    "    cnn.train()\n",
    "    cnnRunningLoss = 0\n",
    "    total = 0\n",
    "    R2 = 0\n",
    "    cnnCorrect=0\n",
    "    total1=0\n",
    "    for i, (inputs, labels) in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device).reshape(-1,512,128)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        # Forward propagation\n",
    "        cnnOutputs = cnn(inputs)\n",
    "        # print(cnnOutputs.shape)\n",
    "        l2_lambda = 0.005\n",
    "        l2_reg = torch.tensor(0.).to(device)\n",
    "        for param in cnn.parameters():\n",
    "            l2_reg += torch.norm(param)\n",
    "        # Backpropagation\n",
    "        cnnLoss = criterion(cnnOutputs, labels)+l2_reg*l2_lambda\n",
    "        cnnLoss.backward()\n",
    "        # Gradient update\n",
    "        optimizer.step()\n",
    "        total += 1\n",
    "        total1+=labels.size(0)\n",
    "        cnnRunningLoss += cnnLoss.item()\n",
    "        _, cnnPredicted = torch.max(cnnOutputs.data, 1)\n",
    "        cnnCorrect += (cnnPredicted == labels).sum().item()\n",
    "    return cnnRunningLoss/total, cnnCorrect/total1\n",
    "\n",
    "def valid_epoch(cnn,device,dataloader,loss_fn):\n",
    "    cnn.eval()\n",
    "    totalLoss = 0\n",
    "    total = 0\n",
    "    total1 = 0\n",
    "    R2 = 0\n",
    "    cnnLoss = 0\n",
    "    cnnCorrect=0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device).reshape(-1,512,128)\n",
    "        labels = labels.to(device)\n",
    "        cnnOutputs = cnn(inputs)\n",
    "        cnnLoss = criterion(cnnOutputs, labels)\n",
    "        _, cnnPredicted = torch.max(cnnOutputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        total1 +=1\n",
    "        totalLoss += cnnLoss.item()\n",
    "        cnnCorrect += (cnnPredicted == labels).sum().item()\n",
    "    accuracy = cnnCorrect / total\n",
    "    cnn.train()\n",
    "    return totalLoss/total1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrgmoWX3jTp9"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "X_train_v, X_test, y_train_v, y_test = train_test_split(X, y, test_size=0.2, random_state=99, stratify=y) #First split, so there is no data leakage problems\n",
    "\n",
    "# Transform data into DataSet\n",
    "X_train_v = torch.as_tensor(X_train_v, dtype=torch.float) # an alternative to torch.from_numpy\n",
    "y_train_v = torch.as_tensor(y_train_v, dtype=torch.float)\n",
    "X_test = torch.as_tensor(X_test, dtype=torch.float)\n",
    "y_test = torch.as_tensor(y_test, dtype=torch.float)\n",
    "splits=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "train_dataset = TensorDataset(X_train_v, y_train_v)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "batch_size=128\n",
    "models = []\n",
    "history = {'fold':[], 'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "\n",
    "#K-Fold validation\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(X_train_v,y_train_v)):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    t = pd.DataFrame(y_train_v[train_idx], columns = ['class']).reset_index()\n",
    "    idx = []\n",
    "    y_train = []\n",
    "    #Resampling to solve the unbalanced problem\n",
    "    for i in range(0,num_classes):\n",
    "        # if i!=label_dict['Unknown']:\n",
    "      idx += list(t[t['class']==i].sample(500,replace=True)['index'])\n",
    "      y_train += [i]*500\n",
    "    X_train = X_train_v[train_idx][idx]\n",
    "\n",
    "    t = pd.DataFrame(y_train_v[val_idx], columns = ['class']).reset_index()\n",
    "    idx = []\n",
    "    y_val = []\n",
    "    for i in range(0,num_classes):\n",
    "        # if i!=label_dict['Unknown']:\n",
    "      idx += list(t[t['class']==i].sample(20,replace=True)['index'])\n",
    "      y_val += [i]*20\n",
    "    X_val =  X_train_v[val_idx][idx]\n",
    "\n",
    "    X_train = torch.as_tensor(X_train, dtype=torch.float) # an alternative to torch.from_numpy\n",
    "    y_train = torch.as_tensor(y_train, dtype=torch.float).type(torch.LongTensor)\n",
    "    X_val = torch.as_tensor(X_val, dtype=torch.float)\n",
    "    y_val = torch.as_tensor(y_val, dtype=torch.float).type(torch.LongTensor)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = TextClassifier(128, embedding_dim=128, hidden_size=1024, num_classes=18).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(list(model.parameters()), lr=learningRate, momentum=momentum)\n",
    "    # optimizer = torch.optim.Adam(list(model.parameters()), lr=learningRate)\n",
    "    best_test = -np.inf\n",
    "    best_model = None\n",
    "\n",
    "    #Train the model\n",
    "    for epoch in range(numEpochs):\n",
    "        train_loss, train_R2=train_epoch(model,device,train_dataloader,criterion,optimizer)\n",
    "        test_loss, test_R2=valid_epoch(model,device,test_dataloader,criterion)\n",
    "        history['fold'].append(fold)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_R2)\n",
    "        history['test_acc'].append(test_R2)\n",
    "        if test_R2>best_test:\n",
    "            test_test = test_R2\n",
    "            best_model = model\n",
    "        print('Train Loss: {}, Train_acc: {}, Test Loss: {}, Test acc: {}'.format(train_loss, train_R2, test_loss, test_R2))\n",
    "    models.append([best_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeNeok2BbRHk"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [[model]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Prc3ht2anMZ6"
   },
   "outputs": [],
   "source": [
    "#Calculate the average accuray of the 5 models\n",
    "X_train_v, X_test, y_train_v, y_test = train_test_split(X, y, test_size=0.2, random_state=99, stratify=y)\n",
    "t = pd.DataFrame(y_test, columns = ['class']).reset_index()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "idx = []\n",
    "y_test = []\n",
    "for i in range(0,num_classes):\n",
    "    # if i!=label_dict['Unknown']:\n",
    "    idx += list(t[t['class']==i].sample(20,replace=True)['index'])\n",
    "    y_test += [i]*20\n",
    "X_test =  X_test[idx]\n",
    "X_test = torch.as_tensor(X_test, dtype=torch.float)\n",
    "y_test = torch.as_tensor(y_test, dtype=torch.float).type(torch.LongTensor)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "acc = []\n",
    "for best_model in models:\n",
    "  acc.append(valid_epoch(best_model[0],device,test_dataloader,criterion)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-N_ChcrG70nP"
   },
   "outputs": [],
   "source": [
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhEUUD5ZgLgM"
   },
   "outputs": [],
   "source": [
    "# Calculate the accuracy with majority vote\n",
    "\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "  for model in models:\n",
    "    model[0].eval()\n",
    "    model_output = []\n",
    "    label_= []\n",
    "    model[0].to(device)\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs = inputs.reshape(-1,512,128).to(device)\n",
    "        labels = labels.to(device)\n",
    "        cnnOutputs = model[0](inputs)\n",
    "        model_output.append(cnnOutputs)\n",
    "        label_.append(labels)\n",
    "        # del inputs\n",
    "    outputs.append(torch.vstack(model_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQew8gS7o7SU"
   },
   "outputs": [],
   "source": [
    "shape = outputs[0].shape\n",
    "labels = torch.hstack(label_)\n",
    "cnnresult = torch.vstack(outputs).reshape(-1,shape[0],shape[1]).mean(axis=0).to('cpu')\n",
    "_, cnnPredicted = torch.max(cnnresult.data, 1)\n",
    "cnnCorrect = (cnnPredicted.detach().numpy() == labels.to('cpu').detach().numpy()).sum().item()\n",
    "print(cnnCorrect/len(labels)*100)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confu_m = confusion_matrix(labels.to('cpu'), cnnPredicted.to('cpu'))\n",
    "import seaborn as sns\n",
    "sns.heatmap(confu_m,square=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1e7wf_Ai0FH4"
   },
   "outputs": [],
   "source": [
    "# Show the Precision, Recall, F1 scores of each classes\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "scores = pd.DataFrame(precision_recall_fscore_support(cnnPredicted.to('cpu'), labels.to('cpu'), labels = list(range(num_classes))),index=['precision','recall','f1','support'], columns = list(label_dict)[:num_classes])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtA905R9OigX"
   },
   "outputs": [],
   "source": [
    "# Print the label_dict\n",
    "label_dict = {}\n",
    "for cl in le.classes_:\n",
    "    label_dict.update({cl:le.transform([cl])[0]})\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3t08d9N6oQgi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
